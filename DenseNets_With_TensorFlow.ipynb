{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNets_With_TensorFlow.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbkXMkrTgXiF"
      },
      "source": [
        "\n",
        "# **From Paper To Keras: DenseNets With TensorFlow**\n",
        "\n",
        "[<img src=\"https://github.com/shubham0204/Privacy_Policy_Texts/blob/master/notebook_button_two.png?raw=true\" width=\"170\" height=\"50\" align=\"center\">](https://medium.com/@equipintelligence/exploring-densenets-from-paper-to-keras-dcc01725488b)\n",
        "[<img src=\"https://github.com/shubham0204/Privacy_Policy_Texts/raw/master/read_the_paper_button.png\" width=\"150\" height=\"40\" align=\"center\">](https://arxiv.org/abs/1608.06993)\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, we'll create the popular [DenseNet](https://arxiv.org/abs/1608.06993) architecture right from scratch! We'll understand the structure right from the beginning and implement it using `tf.keras`.\n",
        "\n",
        "We'll require a GPU Hardware accelerator for training the model. Change the runtime type to GPU by going to `Tools > Change Runtime Type > Hardware Accelerator > GPU`.\n",
        "\n",
        "***Note: It is highly recommended that you go through the research paper once as you'll come across various expressions which are from the paper, in this notebook.***\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnpIO-lThFCl"
      },
      "source": [
        "\n",
        "## 1) Importing the Packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctoxP_8shhPr"
      },
      "source": [
        "\n",
        "We import TensorFlow and NumPy. Other packages are imported as and when needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jso6uFH9-4m"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSBmIS2sh310"
      },
      "source": [
        "\n",
        "## 2) Loading the Data\n",
        "\n",
        "We'll train our model on the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset which can be easily loaded using the `tf.keras.datasets` module.\n",
        "\n",
        "You can try more datasets via [TensorFlow Datasets](https://www.tensorflow.org/datasets) and see the list of image datasets on their [catalog](https://www.tensorflow.org/datasets/catalog/overview).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUvzB_pTMQgC"
      },
      "source": [
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# One-hot encoding for 10 classes.\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQSjglxckPcO"
      },
      "source": [
        "\n",
        "## 3) The DenseNet Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMYXHrWSVEYi"
      },
      "source": [
        "\n",
        "### a) The $H$ function ( $BatchNorm \\to ReLU \\to Conv$ )\n",
        "\n",
        "The namne \"DenseNet\" is enough to give an intuition of this. DenseNet is made of Dense Blocks. \"Blocks\" only refers to a group of different layers.\n",
        "\n",
        "Dense Blocks, rather than traditional CNNs where information ( or feature maps ) flow in a single direction, allow Convolution layers to access inputs from all the previous layers present in the network. The information flows improves as each layer is connected with all its previous layers as well the feature map of the previous layer.\n",
        "\n",
        "In the paper, you'll come across an expression,\n",
        "\n",
        "$x_l = H( x_{l-1} ) + x_{l-1} $\n",
        "\n",
        "$H$ represents a composite function which takes in an image/feature map ( $x$ ) and performs some operations on it. \n",
        "\n",
        "$ x \\to Batch \\ Normalization \\to ReLU \\to Zero \\ Padding \\to 3 \\times 3 \\ Convolution \\ \\to Dropout$\n",
        "\n",
        "\n",
        "The bottleneck layer could be added too. We've implemented it below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La20kIGvY6vg"
      },
      "source": [
        "\n",
        "def H(  inputs, num_filters , dropout_rate ):\n",
        "    x = tf.keras.layers.BatchNormalization( epsilon=eps )( inputs )\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    x = tf.keras.layers.ZeroPadding2D((1, 1))(x)\n",
        "    x = tf.keras.layers.Conv2D(num_filters, kernel_size=(3, 3), use_bias=False , kernel_initializer='he_normal' )(x)\n",
        "    x = tf.keras.layers.Dropout(rate=dropout_rate )(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnlympvRZM74"
      },
      "source": [
        "\n",
        "### b) The Transition Layers\n",
        "\n",
        "The Transition layers perform the downsampling of the feature maps. The feature maps come from the previous block. The compression_factor observed below is the $\\theta$ value from the paper which is the compression factor.\n",
        "\n",
        "Hence, if $m$ feature maps go into the transition layer, we'll produce $[m \\theta]$ feature maps. $[ \\ ]$ represents the floor function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdpD-e3a313p"
      },
      "source": [
        "\n",
        "def transition(inputs, num_filters , compression_factor , dropout_rate ):\n",
        "    # compression_factor is the 'Î¸'\n",
        "    x = tf.keras.layers.BatchNormalization( epsilon=eps )(inputs)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    num_feature_maps = inputs.shape[1] # The value of 'm'\n",
        "\n",
        "    x = tf.keras.layers.Conv2D( np.floor( compression_factor * num_feature_maps ).astype( np.int ) ,\n",
        "                               kernel_size=(1, 1), use_bias=False, padding='same' , kernel_initializer='he_normal' , kernel_regularizer=tf.keras.regularizers.l2( 1e-4 ) )(x)\n",
        "    x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
        "    \n",
        "    x = tf.keras.layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq9vuSf3340S"
      },
      "source": [
        "\n",
        "### c) Finally, the Dense Block\n",
        "\n",
        "Each block will get some feature maps as input  from the previous transition layer. These inputs will then go through the $H$ function to produce an output ( $x_1$ ) .\n",
        "\n",
        "$x_1 = H( x_{0} )$\n",
        "\n",
        "Now, $x_1$ again goes into the $H$ function. But this time, its concatenated with $x_{0}$. So $x_2$ will be produced like,\n",
        "\n",
        "$x_2 = H( \\ concat( \\ x_1 , x_0 \\ ) \\ )$\n",
        "\n",
        "Similarly, $x_l$ will be produced by the concatenation of all output feature maps of the previous layers ( as well as the inputs $x_0$ )\n",
        "\n",
        "$x_l = H( \\ concat( \\ x_0 \\ , x_1\\  , \\ x_2 , ... , \\ x_{l-1} \\  ) \\ )$\n",
        "\n",
        "\n",
        "After getting $x_l$,  it will be passed through the transition layer. From then onwards, the outputs of the transition layer again flow in another block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP5qfDbA6zsH"
      },
      "source": [
        "\n",
        "def dense_block( inputs, num_layers, num_filters, growth_rate , dropout_rate ):\n",
        "    for i in range(num_layers): # num_layers is the value of 'l'\n",
        "        conv_outputs = H(inputs, num_filters , dropout_rate )\n",
        "        inputs = tf.keras.layers.Concatenate()([conv_outputs, inputs])\n",
        "        num_filters += growth_rate # To increase the number of filters for each layer.\n",
        "    return inputs, num_filters\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CduWgvUe65-m"
      },
      "source": [
        "\n",
        "We'll add Dense Blocks and Transition layers one after the other. A `GlobalAveragePooling2D` layer ensures that the outputs are 2D and finally a softmax layer produces the class probabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNVG7DI_7HwW"
      },
      "source": [
        "\n",
        "input_shape = ( 32 , 32 , 3 ) \n",
        "num_blocks = 3\n",
        "num_layers_per_block = 4\n",
        "growth_rate = 16\n",
        "dropout_rate = 0.4\n",
        "compress_factor = 0.5\n",
        "eps = 1.1e-5\n",
        "\n",
        "num_filters = 16\n",
        "\n",
        "inputs = tf.keras.layers.Input( shape=input_shape )\n",
        "x = tf.keras.layers.Conv2D( num_filters , kernel_size=( 3 , 3 ) , use_bias=False, kernel_initializer='he_normal' , kernel_regularizer=tf.keras.regularizers.l2( 1e-4 ) )( inputs )\n",
        "\n",
        "for i in range( num_blocks ):\n",
        "    x, num_filters = dense_block( x, num_layers_per_block , num_filters, growth_rate , dropout_rate )\n",
        "    x = transition(x, num_filters , compress_factor , dropout_rate )\n",
        "\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()( x ) \n",
        "x = tf.keras.layers.Dense( 10 )( x ) # Num Classes for CIFAR-10\n",
        "outputs = tf.keras.layers.Activation( 'softmax' )( x )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c02qoDjq7igh"
      },
      "source": [
        "\n",
        "Everything compiled into a beautiful `tf.keras.models.Model`!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGkSJI41fHfg"
      },
      "source": [
        "\n",
        "model = tf.keras.models.Model( inputs , outputs )\n",
        "model.compile( loss=tf.keras.losses.categorical_crossentropy ,optimizer=tf.keras.optimizers.Adam( lr=0.0001 ) ,metrics=[ 'acc' ])\n",
        "model.summary()\n",
        "\n",
        "#Comment out the below line if you want to have an image of your model's structure.\n",
        "\n",
        "#tf.keras.utils.plot_model( model , show_shapes=True )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQwFVM-SlQV-"
      },
      "source": [
        "\n",
        "## 4) Training the Model\n",
        "\n",
        "We'll train the model now.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkZRWOph7n5a"
      },
      "source": [
        "\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "\n",
        "model.fit( x_train , y_train , epochs=epochs , batch_size=batch_size , validation_data=( x_test , y_test ) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfigqcz1BqlA"
      },
      "source": [
        "\n",
        "## 5) Evaluate the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsWsGrm8BvgN"
      },
      "source": [
        "\n",
        "results = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print( 'Loss = {} and Accuracy = {} %'.format( results[0] , results[1] * 100 ) )\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}