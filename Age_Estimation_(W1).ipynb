{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Age_Estimation_(W1).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2bkkoSFDBre"
      },
      "source": [
        "\n",
        "# **ğŸ˜ Age and Gender Estimation in TensorFlow ( Workbook 1, Age Estimation )**\n",
        "\n",
        "* For Gender classification, go to [Workbook 2 ( Gender Classification )](https://colab.research.google.com/drive/1aOMbQYK2bvedNVXkZvHC_FsBsKrCx01P?usp=sharing)\n",
        "\n",
        "In this notebook, we train a Keras model to estimate the age of a person, given a *face-cropped* image. We use the famous [UTKFace Dataset](https://susanqq.github.io/UTKFace/), which contains 23K images where each image is labelled with its gender, age and ethinicity.\n",
        "\n",
        "\n",
        "> **Note: Please make sure that you are connected to the GPU runtime of Google Colab. Else, the training might take a decade long. Go to Runtime > Change runtime type > Hardware accelerator.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1t1Vq_ZEiuq"
      },
      "source": [
        "\n",
        "## 1) **Downloading the UTKFace dataset** ğŸ’»\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEKk6BWqn5nS"
      },
      "source": [
        "The UTKFace dataset is available as a Google Drive folder. See the file [here](https://drive.google.com/drive/folders/0BxYys69jI14kU0I1YUQyY1ZDRUE). Download it place it in your Google Drive.\n",
        "\n",
        "Once completed, mount the Google Drive in this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UquFbQ1817U"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvELTRlHdTnx"
      },
      "source": [
        "\n",
        "Unzip the `utkface_23k.zip` file to the local directory. This would significantly increase the pace at which images are parsed. See [this](https://stackoverflow.com/a/60802388/10878733) answer on StackOverflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzwQHuZsiAsr"
      },
      "source": [
        "\n",
        "# Replace with your path!\n",
        "!unzip -q /content/drive/MyDrive/Datasets/utkface_23k.zip -d data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3sqf_DQdvOF"
      },
      "source": [
        "\n",
        "Rename the directory from `utkace_23k` to `utkface23k`, in order to remove the underscore, which might cause errors while we're splitting the filename, to get the age of the person.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEptd_PoO6Qz"
      },
      "source": [
        "\n",
        "!mv data/utkface_23k data/utkface23k\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb_CNn_ui6QT"
      },
      "source": [
        "\n",
        "# To download checkpoints, Keras models, TFLite models\n",
        "from google.colab import files\n",
        "\n",
        "# Life is incomplete without this statement!\n",
        "import tensorflow as tf\n",
        "\n",
        "# And this as well!\n",
        "import numpy as np\n",
        "\n",
        "# To visualize results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import datetime\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLOWdGmrEg1T"
      },
      "source": [
        "\n",
        "## 2) **Processing the data** ğŸ¦¾\n",
        "\n",
        "Once we've downloaded the dataset, we need to perform the following operations on the dataset, so that it can be used for training our model,\n",
        "\n",
        "* ğŸ‘‰ğŸ» Reading the image files as 3D NumPy arrays. Note, we'll use 3-channeled RGB images for training the model, so each array will have a shape of `[ img_width , img_height , 3 ]`.\n",
        "\n",
        "* ğŸ‘‰ğŸ» Split the filename so as to parse the age of the person in corresponding image. We use the `tf.strings.split()` method for performing this task.\n",
        "\n",
        "* ğŸ‘‰ğŸ» The maximum value of our target variable is $116$ years which is used for normalizing the `age` variable.\n",
        "\n",
        "\n",
        "Once this operations have been performed, we are left with $N$ samples where each sample consists of image array `[ 200 , 200 , 3 ]` and its corresponding label, the age of that person, which has a shape `[ 1 , ]`\n",
        "\n",
        "We'll use `tf.data.Dataset` as it helps us to process the data faster, taking advantage of parallel computing. The above two operations will be mapped on each filename using `tf.data.Dataset.map` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m67XqZMHirhz"
      },
      "source": [
        "\n",
        "# Image size for our model.\n",
        "MODEL_INPUT_IMAGE_SIZE = [ 200 , 200 ]\n",
        "\n",
        "# Fraction of the dataset to be used for testing.\n",
        "TRAIN_TEST_SPLIT = 0.3\n",
        "\n",
        "# Number of samples to take from dataset\n",
        "N = 20000\n",
        "\n",
        "# This method will be mapped for each filename in `list_ds`. \n",
        "def parse_image( filename ):\n",
        "\n",
        "    # Read the image from the filename and resize it.\n",
        "    image_raw = tf.io.read_file( filename )\n",
        "    image = tf.image.decode_jpeg( image_raw , channels=3 ) \n",
        "    image = tf.image.resize( image , MODEL_INPUT_IMAGE_SIZE ) / 255\n",
        "\n",
        "    # Split the filename to get the age and the gender. Convert the age ( str ) and the gender ( str ) to dtype float32.\n",
        "    parts = tf.strings.split( tf.strings.split( filename , '/' )[ 2 ] , '_' )\n",
        "\n",
        "    # Normalize\n",
        "    age = tf.strings.to_number( parts[ 0 ] ) / 116\n",
        "\n",
        "    return image , age\n",
        "\n",
        "# List all the image files in the given directory.\n",
        "list_ds = tf.data.Dataset.list_files( 'data/utkface23k/*' , shuffle=True )\n",
        "\n",
        "# Map `parse_image` method to all filenames.\n",
        "dataset = list_ds.map( parse_image , num_parallel_calls=tf.data.AUTOTUNE )\n",
        "dataset = dataset.take( N )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYSpbniQHLnm"
      },
      "source": [
        "\n",
        "We create two splits from our dataset, one for training the model and another for testing the model. The fraction of the dataset which will be used for testing the model is determined by `TRAIN_TEST_SPLIT`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9aO3SH22mFK"
      },
      "source": [
        "\n",
        "# Create train and test splits of the dataset.\n",
        "num_examples_in_test_ds = int( dataset.cardinality().numpy() * TRAIN_TEST_SPLIT )\n",
        "\n",
        "test_ds = dataset.take( num_examples_in_test_ds )\n",
        "train_ds = dataset.skip( num_examples_in_test_ds )\n",
        "\n",
        "print( 'Num examples in train ds {}'.format( train_ds.cardinality() ) )\n",
        "print( 'Num examples in test ds {}'.format( test_ds.cardinality() ) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdCOHkLQphjz"
      },
      "source": [
        "\n",
        "## 3) **The CNN Model** ğŸ‘¨â€ğŸ“\n",
        "\n",
        "In our approach, we treat age estimation as a regression problem. Our aim is to develop a model which has lesser parameters ( which implies lesser inference time and size ) but powerful enough so that it can generalize better.\n",
        "\n",
        "* ğŸ‘‰ğŸ» The model takes in a batch of shape `[ None , 200 , 200 , 3 ]` and performs a number of convolutions on it as determined by `num_blocks`.\n",
        "* ğŸ‘‰ğŸ» Each block consists of a sequence of layers : `Conv2D -> BatchNorm -> LeakyReLU`\n",
        "\n",
        "```\n",
        "# Define the conv block.\n",
        "if lite_model:\n",
        "        x = tf.keras.layers.SeparableConv2D( num_filters ,\n",
        "                                            kernel_size=kernel_size ,\n",
        "                                            strides=strides \n",
        "                                            , use_bias=False ,\n",
        "                                            kernel_initializer=tf.keras.initializers.HeNormal() ,\n",
        "                                            kernel_regularizer=tf.keras.regularizers.L2( 1e-5 )\n",
        "                                             )( x )\n",
        "    else:\n",
        "        x = tf.keras.layers.Conv2D( num_filters ,\n",
        "                                   kernel_size=kernel_size ,\n",
        "                                   strides=strides ,\n",
        "                                   use_bias=False ,\n",
        "                                   kernel_initializer=tf.keras.initializers.HeNormal() ,\n",
        "                                   kernel_regularizer=tf.keras.regularizers.L2( 1e-5 )\n",
        "                                    )( x )\n",
        "\n",
        "    x = tf.keras.layers.BatchNormalization()( x )\n",
        "    x = tf.keras.layers.LeakyReLU( leaky_relu_alpha )( x )\n",
        "```\n",
        "\n",
        "* If `lite_model` is set to `True`, we use [Separable Convolutions](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728) which have lesser parameters. We could achieve a *faster* model, compromising its performance.\n",
        "\n",
        "* ğŸ‘‰ğŸ» We stack such`num_blocks` blocks sequentially, where the no. of filters for each layer is taken from `num_filters`.\n",
        "\n",
        "* ğŸ‘‰ğŸ» Next we add a number of `Dense` layers to learn the features extracted by convolutional layers. Note, we also add a `Dropout` layer, to reduce overfitting. The `rate` for each `Dropout` layer is decreased subsequently for each layer, so that the learnability of `Dense` layer with lesser units ( neurons ) is not affected.\n",
        "\n",
        "```\n",
        "def dense( x , filters , dropout_rate ):\n",
        "    x = tf.keras.layers.Dense( filters , kernel_regularizer=tf.keras.regularizers.L2( 0.1 ) , bias_regularizer=tf.keras.regularizers.L2( 0.1 ) )( x )\n",
        "    x = tf.keras.layers.LeakyReLU( alpha=leaky_relu_alpha )( x )\n",
        "    x = tf.keras.layers.Dropout( dropout_rate )( x )\n",
        "    return x\n",
        "```\n",
        "\n",
        "> See [this](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/) blog for choosing the weight decay values used in the above two blocks.\n",
        "\n",
        "* ğŸ‘‰ğŸ» The output of the model is a tensor with shape `[ None, 1 ]`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qmw6DHVG2QD"
      },
      "source": [
        "\n",
        "# Negative slope coefficient for LeakyReLU.\n",
        "leaky_relu_alpha = 0.2\n",
        "\n",
        "lite_model = False\n",
        "\n",
        "# Define the conv block.\n",
        "def conv( x , num_filters , kernel_size=( 3 , 3 ) , strides=1 ):\n",
        "    if lite_model:\n",
        "        x = tf.keras.layers.SeparableConv2D( num_filters ,\n",
        "                                            kernel_size=kernel_size ,\n",
        "                                            strides=strides, \n",
        "                                            use_bias=False ,\n",
        "                                            kernel_initializer=tf.keras.initializers.HeNormal() ,\n",
        "                                            kernel_regularizer=tf.keras.regularizers.L2( 1e-5 )\n",
        "                                             )( x )\n",
        "    else:\n",
        "        x = tf.keras.layers.Conv2D( num_filters ,\n",
        "                                   kernel_size=kernel_size ,\n",
        "                                   strides=strides ,\n",
        "                                   use_bias=False ,\n",
        "                                   kernel_initializer=tf.keras.initializers.HeNormal() ,\n",
        "                                   kernel_regularizer=tf.keras.regularizers.L2( 1e-5 )\n",
        "                                    )( x )\n",
        "\n",
        "    x = tf.keras.layers.BatchNormalization()( x )\n",
        "    x = tf.keras.layers.LeakyReLU( leaky_relu_alpha )( x )\n",
        "    return x\n",
        "\n",
        "def dense( x , filters , dropout_rate ):\n",
        "    x = tf.keras.layers.Dense( filters , kernel_regularizer=tf.keras.regularizers.L2( 0.1 ) , bias_regularizer=tf.keras.regularizers.L2( 0.1 ) )( x )\n",
        "    x = tf.keras.layers.LeakyReLU( alpha=leaky_relu_alpha )( x )\n",
        "    x = tf.keras.layers.Dropout( dropout_rate )( x )\n",
        "    return x\n",
        "\n",
        "\n",
        "# No. of convolution layers to be added.\n",
        "num_blocks = 6\n",
        "# Num filters for each conv layer.\n",
        "num_filters = [ 16 , 32 , 64 , 128 , 256 , 256 ]\n",
        "# Kernel sizes for each conv layer.\n",
        "kernel_sizes = [ 3 , 3 , 3 , 3 , 3 , 3 ]\n",
        "\n",
        "# Init a Input Layer.\n",
        "inputs = tf.keras.layers.Input( shape=MODEL_INPUT_IMAGE_SIZE + [ 3 ] )\n",
        "\n",
        "# Add conv blocks sequentially\n",
        "x = inputs\n",
        "for i in range( num_blocks ):\n",
        "    x = conv( x , num_filters=num_filters[ i ] , kernel_size=kernel_sizes[ i ] )\n",
        "    x = tf.keras.layers.MaxPooling2D()( x )\n",
        "\n",
        "# Flatten the output of the last Conv layer.\n",
        "x = tf.keras.layers.Flatten()( x )\n",
        "conv_output = x \n",
        "\n",
        "# Add Dense layers ( Dense -> LeakyReLU -> Dropout )\n",
        "x = dense( conv_output , 256 , 0.6 )\n",
        "x = dense( x , 64 , 0.4 )\n",
        "x = dense( x , 32 , 0.2 )\n",
        "outputs = tf.keras.layers.Dense( 1 , activation='relu' )( x )\n",
        "\n",
        "# Build the Model\n",
        "model = tf.keras.models.Model( inputs , outputs )\n",
        "\n",
        "# Uncomment the below to view the summary of the model.\n",
        "model.summary()\n",
        "# tf.keras.utils.plot_model( model , to_file='architecture.png' )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrHarErGuFKa"
      },
      "source": [
        "\n",
        "## 4) **Compiling the model ( and other callbacks )** ğŸ§±\n",
        "\n",
        "Once we've defined the architecture for our model, we'll compile our Keras model and also initialize some useful callbacks.\n",
        "\n",
        "* ğŸ‘‰ğŸ» As we're performing regression, we'll use the Mean Absolute Error ( MAE ) loss function. See [`tf.keras.losses.mean_absolute_error`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MAE) for more details.\n",
        "\n",
        "* ğŸ‘‰ğŸ» We'll use the Adam optimizer for training our model. See [`tf.keras.optimizers.Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) for more details.\n",
        "\n",
        "* ğŸ‘‰ğŸ» For evaluating the performance of our model, we use Mean Absolute Error as a metric. See [`tf.keras.metrics.MeanAbsoluteError`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsoluteError) for more details.\n",
        "\n",
        "\n",
        "#### Callbacks:\n",
        "\n",
        "* ğŸ‘‰ğŸ» [`tf.keras.callbacks.ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to save the Keras model as an H5 file after every epoch.\n",
        "\n",
        "* ğŸ‘‰ğŸ» [`tf.keras.callbacks.TensorBoard`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) to visualize the training with TensorBoard.\n",
        "\n",
        "* ğŸ‘‰ğŸ» [`tf.keras.callbacks.LearningRateScheduler`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler) to decrease the learning rate over a certain number of epochs, so as to make smaller steps, as the optimizer reaches near the minima of the loss function.\n",
        "\n",
        "```\n",
        "def scheduler( epochs , learning_rate ):\n",
        "    if epochs < num_epochs * 0.25:\n",
        "        return learning_rate\n",
        "    elif epochs < num_epochs * 0.5:\n",
        "        return 0.0005\n",
        "    elif epochs < num_epochs * 0.75:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.000095\n",
        "```\n",
        "\n",
        "* ğŸ‘‰ğŸ» [`tf.keras.callbacks.EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) to stop the training when the evaluation metric i.e the MAE stops improving on the test dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbvs6byaspdK"
      },
      "source": [
        "\n",
        "# Initial learning rate\n",
        "learning_rate = 0.001\n",
        "\n",
        "num_epochs = 50 #@param {type: \"number\"}\n",
        "batch_size = 128 #@param {type: \"number\"}\n",
        "\n",
        "# Batch and repeat `train_ds` and `test_ds`.\n",
        "train_ds = train_ds.batch( batch_size )\n",
        "test_ds = test_ds.batch( batch_size )\n",
        "\n",
        "# Init ModelCheckpoint callback\n",
        "save_dir_ = 'model_1'  #@param {type: \"string\"}\n",
        "save_dir = save_dir_ + '/{epoch:02d}-{val_mae:.2f}.h5'\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint( \n",
        "    save_dir , \n",
        "    save_best_only=True , \n",
        "    monitor='val_mae' , \n",
        "    mode='min' , \n",
        ")\n",
        "\n",
        "tb_log_name = 'model_1'  #@param {type: \"string\"}\n",
        "# Init TensorBoard Callback\n",
        "logdir = os.path.join( \"tb_logs\" , tb_log_name )\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard( logdir )\n",
        "\n",
        "# Init LR Scheduler\n",
        "def scheduler( epochs , learning_rate ):\n",
        "    if epochs < num_epochs * 0.25:\n",
        "        return learning_rate\n",
        "    elif epochs < num_epochs * 0.5:\n",
        "        return 0.0005\n",
        "    elif epochs < num_epochs * 0.75:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.000095\n",
        "\n",
        "lr_schedule_callback = tf.keras.callbacks.LearningRateScheduler( scheduler )\n",
        "\n",
        "# Init Early Stopping callback\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping( monitor='val_mae' , patience=10 )\n",
        "\n",
        "# Compile the model\n",
        "model.compile( \n",
        "    loss=tf.keras.losses.mean_absolute_error ,\n",
        "    optimizer = tf.keras.optimizers.Adam( learning_rate ) , \n",
        "    metrics=[ 'mae' ]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCVYNawQjXRj"
      },
      "source": [
        "\n",
        "Run this cell to visualize the training of the model in TensorBoard ( in this notebook itself ).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycIUYrp65w7r"
      },
      "source": [
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tb_logs/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M-KwmgEj0y1"
      },
      "source": [
        "\n",
        "## 5) **Train and Evaluate the Model** ğŸ‹ğŸ»â€â™‚ï¸\n",
        "\n",
        "Start the training loop with all callbacks packed in.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cow71DK6kjUQ"
      },
      "source": [
        "\n",
        "model.fit( \n",
        "    train_ds, \n",
        "    epochs=num_epochs,  \n",
        "    validation_data=test_ds, \n",
        "    callbacks=[ checkpoint_callback , tensorboard_callback , lr_schedule_callback , early_stopping_callback ]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGfVFGeXj7Rj"
      },
      "source": [
        "Evaluate the Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZKpG-IVBr_C"
      },
      "source": [
        "\n",
        "p = model.evaluate( test_ds )\n",
        "print( p )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDrTINwcMXGQ"
      },
      "source": [
        "( Optional ) Load a .h5 file for evaluation. Note: You need to run the code cell which creates `train_ds` and `test_ds`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIm_gNWnMSUk"
      },
      "source": [
        "\n",
        "batch_size = 128\n",
        "\n",
        "model = tf.keras.models.load_model( 'model_lite_age.h5' )\n",
        "p = model.evaluate( test_ds.batch( batch_size ) )\n",
        "print( p )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DmFXTlOlVKE"
      },
      "source": [
        "\n",
        "Save the Keras model to the local disk, so that we can resume training if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5TtLuHZDFry"
      },
      "source": [
        "\n",
        "model_name = 'model_age' #@param {type: \"string\"}\n",
        "model_name_ = model_name + '.h5'\n",
        "\n",
        "model.save( model_name_ )\n",
        "files.download( model_name_ ) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x28Xr22f2Hny"
      },
      "source": [
        "\n",
        "## 6) **Visualize the results**\n",
        "\n",
        "We'll predict the age from some images taken from `test_ds` and plot them using `matplotlib`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kiV_xRc2QzD"
      },
      "source": [
        "\n",
        "fig = plt.figure( figsize=( 10 , 15 ) )\n",
        "rows = 5\n",
        "columns = 2\n",
        "\n",
        "i = 1\n",
        "for image , label in test_ds.unbatch().take( 10 ):\n",
        "    image = image.numpy()\n",
        "    fig.add_subplot( rows , columns , i )\n",
        "    plt.imshow( image )\n",
        "    label_ = int( model.predict( np.expand_dims( image , 0 ) ) * 116 )\n",
        "    plt.axis( 'off' )\n",
        "    plt.title( 'Predicted age : {} , actual age : {}'.format( label_ , int( label.numpy() * 116 ) ) )\n",
        "    i += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSOFJqsf-uK9"
      },
      "source": [
        " \n",
        "## 7) **Convert to TensorFlow Lite format** ğŸ“¡\n",
        "\n",
        "Our model is to be deployed in an Android app, where we'll use [TF Lite Android](https://bintray.com/google/tensorflow/tensorflow-lite) package to parse the model and make predictions.\n",
        "\n",
        "We use the `TFLiteConverter` API to convert our Keras Model ( `.h5` ) to a TF Lite buffer ( `.tflite` ). See the [official docs](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter/). We'll produce two TF Lite buffers, one with float16 quantization and other non-quantized model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwmMTQxyhToJ"
      },
      "source": [
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model( model )\n",
        "converter.optimizations = [ tf.lite.Optimize.DEFAULT ]\n",
        "converter.target_spec.supported_types = [ tf.float16 ]\n",
        "buffer = converter.convert()\n",
        "\n",
        "open( '{}_q.tflite'.format( model_name ) , 'wb' ).write( buffer )\n",
        "files.download( '{}_q.tflite'.format( model_name ) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5VTeSnGAhm_"
      },
      "source": [
        "\n",
        "For conversion to a non-quantized TF Lite buffer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jnurapqK6fw"
      },
      "source": [
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model( model )\n",
        "buffer = converter.convert()\n",
        "\n",
        "open( '{}_nonq.tflite'.format( model_name ) , 'wb' ).write( buffer )\n",
        "files.download( '{}_nonq.tflite'.format( model_name ) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uQcN2eMkrR5"
      },
      "source": [
        "\n",
        "## Utility Methods\n",
        "\n",
        "Use these methods to automate some of the tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGGdHh5oEtK1",
        "cellView": "form"
      },
      "source": [
        "\n",
        "#@title Utility to zip and download a directory\n",
        "#@markdown Use this method to zip and download a directory. For ex. a TB logs \n",
        "#@markdown directory or a checkpoint(s) directory.\n",
        "\n",
        "dir_to_zip = 'tb_logs' #@param {type: \"string\"}\n",
        "output_filename = 'logs.zip' #@param {type: \"string\"}\n",
        "delete_dir_after_download = \"No\"  #@param ['Yes', 'No']\n",
        "\n",
        "os.system( \"zip -r {} {}\".format( output_filename , dir_to_zip ) )\n",
        "\n",
        "if delete_dir_after_download == \"Yes\":\n",
        "    os.system( \"rm -r {}\".format( dir_to_zip ) )\n",
        "\n",
        "files.download( output_filename )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGbWLjNXJMSt",
        "cellView": "form"
      },
      "source": [
        "\n",
        "#@title Utility to delete a directory\n",
        "#@markdown Use this method to delete a directory. \n",
        "\n",
        "dir_path = ''  #@param {type: \"string\"}\n",
        "os.system( f'rm -r {dir_path}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYZ-t7k1quXK"
      },
      "source": [
        "\n",
        "# LICENSE\n",
        "\n",
        "```\n",
        "  \n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2021 Shubham Panchal\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "```\n"
      ]
    }
  ]
}