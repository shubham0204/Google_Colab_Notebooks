{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyperparameter_Optimization_Using_KerasTuner.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzvIjH2IDFk3"
      },
      "source": [
        "\n",
        "# Hyperparameter Optimization Using Keras Tuner API\n",
        "\n",
        "[Hyperparameter optimization](https://www.jeremyjordan.me/hyperparameter-tuning/) is important if you're trying to make a model state-of-the-art. For instance, if you're developing a new architecture for image classification, you'll like to set such a value for the number of output units ( output dimensionality ) which would give you convergence quickly during training. Accuracy also depends hugely on hyperparameters ( like batch-size, learning rate, weight decay ).\n",
        "\n",
        "We'll be using the [Keras Tuner API](https://keras-team.github.io/keras-tuner/) which hyperparameter optimization right in our TF Keras models and that too in an easy-to-use interface.\n",
        "\n",
        "You may read the [TensorFlow Blog](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html) and [official tutorial](https://www.tensorflow.org/tutorials/keras/keras_tuner#overview).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIz5x3SUER1H"
      },
      "source": [
        "\n",
        "## 1) Import TensorFlow and install kerastuner via pip\n",
        "\n",
        "We'll install `kerastuner` package using pip.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97eaOhzYCzgP"
      },
      "source": [
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import IPython\n",
        "\n",
        "!pip install keras-tuner\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cypilEJTEse8"
      },
      "source": [
        "\n",
        "## 2) Preparing the CIFAR10 dataset for training the model\n",
        "\n",
        "We'll require some data to train and optimize our model. We use the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset as it's readily available in `tf.keras.datasets` module.\n",
        "Also, we normalize the pixel values between 0 and 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULiTNODDEqkz"
      },
      "source": [
        "\n",
        "( train_img, train_label ), ( test_img , test_label ) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "train_img = train_img.astype( 'float32' ) / 255\n",
        "test_img = test_img.astype( 'float32' ) / 255\n",
        "\n",
        "train_label = keras.utils.to_categorical( train_label , num_classes=10 ) \n",
        "test_label = keras.utils.to_categorical( test_label , num_classes=10 ) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XRYO2duGFB_"
      },
      "source": [
        "\n",
        "## 3) Define a CNN model with hyperparameters\n",
        "\n",
        "We'll use a Convolutional Neural Network to classify images of the CIFAR10 dataset. We'll use two Conv2D layers and optimize their `units` argument. This method creates a new model with the given hyperparameter ( `hp` ).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gsJDTwCF9St"
      },
      "source": [
        "\n",
        "import kerastuner as kt\n",
        "\n",
        "def model( hp ):\n",
        "    model = keras.Sequential()\n",
        "    hp_units = hp.Int( 'units', min_value=32, max_value=512, step=32)\n",
        "    model.add( keras.layers.Conv2D( hp_units , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' , input_shape=( 32 , 32 , 3 ) ) )\n",
        "    model.add( keras.layers.Conv2D( hp_units , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' , input_shape=( 32 , 32 , 3 ) ) )\n",
        "    model.add( keras.layers.Flatten() )\n",
        "    tf.keras.activations.relu\n",
        "    activation = hp.Choice( 'activation' , values=[ 'selu' , 'relu' , 'leaky_relu' ] )\n",
        "    model.add( keras.layers.Activation( activation ) )\n",
        "    model.add( keras.layers.Dense( 10 , activation='softmax' ) )\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
        "    model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
        "                loss='categorical_crossentropy', \n",
        "                metrics = ['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXvAtsm3HSfo"
      },
      "source": [
        "\n",
        "Intialize the `Tuner`. We'll use [Hyperband algorithm](https://arxiv.org/pdf/1603.06560.pdf) for optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fONcRR3XHa5_"
      },
      "source": [
        "\n",
        "tuner = kt.tuners.Hyperband( model,\n",
        "                     objective='val_accuracy', \n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='opt_dir',\n",
        "                     project_name='cifar10CNNopt' )      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGjO2ShmIeTI"
      },
      "source": [
        "\n",
        "## 4) Running the `tuner` for optimization\n",
        "\n",
        "We supply the training and testing data to the tuner for hyperparamter optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL6a-w1ZIsUV"
      },
      "source": [
        "\n",
        "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
        "  def on_train_end(*args, **kwargs):\n",
        "    IPython.display.clear_output(wait = True)\n",
        "\n",
        "tuner.search( train_img , train_label , epochs=15 , validation_data=( test_img , test_label ), callbacks = [ClearTrainingOutput()])\n",
        "\n",
        "# Optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUo-CyrmJ3gG"
      },
      "source": [
        "\n",
        "Once the optimization is done, take the best parameters and load our model with them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie0SvOdIJ-E2"
      },
      "source": [
        "\n",
        "# Load the optimal parameters\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Train the model using them\n",
        "model.fit(img_train, label_train, epochs = 10, validation_data = (img_test, label_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyzhdFPNKOfl"
      },
      "source": [
        "\n",
        "That's All! Keras just became more great!\n"
      ]
    }
  ]
}