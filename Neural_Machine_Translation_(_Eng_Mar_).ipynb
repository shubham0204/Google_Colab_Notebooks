{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Machine_Translation_( Eng-Mar ).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHBP5xzmrM4U"
      },
      "source": [
        "# Neural Machine Translation (NMT) - Translating English sentences to Marathi sentences\n",
        "\n",
        "Machine Translation refers to translating phrases across languages using deep learning and specifically with RNN ( Recurrent Neural Nets ). Most of these are complex systems that is they are a combined system of various algorithms. But, at its core, NMT uses sequence-to-sequence ( seq2seq ) RNN cells. Such models could be character level but word level models remain common.\n",
        "\n",
        "![NMT system](https://3.bp.blogspot.com/-3Pbj_dvt0Vo/V-qe-Nl6P5I/AAAAAAAABQc/z0_6WtVWtvARtMk0i9_AtLeyyGyV6AI4wCLcB/s1600/nmt-model-fast.gif)\n",
        "\n",
        "I insist to change the runtime to a GPU runtime so that training could be faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVPRC0bOtBas"
      },
      "source": [
        "## What are we going to do?\n",
        "We will basically create an encoder-decoder LSTM model using [Keras Functional API](https://www.tensorflow.org/alpha/guide/keras/functional) ( with [TensorFlow](https://www.tensorflow.org/) ). We will convert the English sentences to [Marathi](https://en.wikipedia.org/wiki/Marathi_language) ( A language native to India ). But, why Marathi?\n",
        "\n",
        "\n",
        "*   Has special characters and much complex.\n",
        "*   Has a totally new script ( Devnagiri ) with no pretrained word-embeddings available yet.\n",
        "\n",
        "Here's an example,\n",
        "\n",
        "the cat sleeps among the dogs  ->  मांजर कुत्रींमध्ये झोपतात\n",
        "\n",
        "So, let's get started.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_r70epHozOt"
      },
      "source": [
        "## Preparing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq4aH4u1uq5V"
      },
      "source": [
        "### 1) Importing the libraries\n",
        "\n",
        "We will import TensorFlow and Keras. From Keras, we import various modules which help in building NN layers, preprocess data and construct LSTM models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK2TWV1nm48Q"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnIg8HdTGW4o"
      },
      "source": [
        "### 2) Reading the data\n",
        "\n",
        "\n",
        "Our dataset which contains more than 30K pairs of English-Marathi phrases. This amazing dataset is available at http://www.manythings.org/anki/ and it also other 50+ sets of bilingual sentences. We download the dataset for English-Marathi phrases, unzip it and read it using [Pandas](https://pandas.pydata.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27OzmS-MIymc"
      },
      "source": [
        "\n",
        "!wget http://www.manythings.org/anki/mar-eng.zip -O mar-eng.zip\n",
        "!unzip mar-eng.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGmBbVVTrm74"
      },
      "source": [
        "\n",
        "lines = pd.read_table( 'mar.txt' , names=[ 'eng' , 'mar' ] )\n",
        "lines.reset_index( level=0 , inplace=True )\n",
        "lines.rename( columns={ 'index' : 'eng' , 'eng' : 'mar' , 'mar' : 'c' } , inplace=True )\n",
        "lines = lines.drop( 'c' , 1 )\n",
        "lines = lines.iloc[ 10000 : 20000 ] \n",
        "lines.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dgIdfjIRLDN"
      },
      "source": [
        "### 3) Preparing input data for the Encoder ( `encoder_input_data` )\n",
        "The Encoder model will be fed input data which are preprocessed English sentences. The preprocessing is done as follows :\n",
        "\n",
        "\n",
        "1.   Tokenizing the English sentences from `eng_lines`.\n",
        "2.   Determining the maximum length of the English sentence that's `max_input_length`.\n",
        "3.   Padding the `tokenized_eng_lines` to the max_input_length.\n",
        "4.   Determining the vocabulary size ( `num_eng_tokens` ) for English words.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2_ux1rZnDyY"
      },
      "source": [
        "\n",
        "eng_lines = list()\n",
        "for line in lines.eng:\n",
        "    eng_lines.append( line ) \n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( eng_lines ) \n",
        "tokenized_eng_lines = tokenizer.texts_to_sequences( eng_lines ) \n",
        "\n",
        "length_list = list()\n",
        "for token_seq in tokenized_eng_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_input_length = np.array( length_list ).max()\n",
        "print( 'English max length is {}'.format( max_input_length ))\n",
        "\n",
        "padded_eng_lines = preprocessing.sequence.pad_sequences( tokenized_eng_lines , maxlen=max_input_length , padding='post' )\n",
        "encoder_input_data = np.array( padded_eng_lines )\n",
        "print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))\n",
        "\n",
        "eng_word_dict = tokenizer.word_index\n",
        "num_eng_tokens = len( eng_word_dict )+1\n",
        "print( 'Number of English tokens = {}'.format( num_eng_tokens))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRwAd310SPkG"
      },
      "source": [
        "### 4) Preparing input data for the Decoder ( `decoder_input_data` )\n",
        "The Decoder model will be fed the preprocessed Marathi lines. The preprocessing steps are similar to the ones which are above. This one step is carried out before the other steps.\n",
        "\n",
        "\n",
        "*   Append `<START>` tag at the first position in  each Marathi sentence.\n",
        "*   Append `<END>` tag at the last position in  each Marathi sentence.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deB0oX_0pj8R"
      },
      "source": [
        "\n",
        "mar_lines = list()\n",
        "for line in lines.mar:\n",
        "    mar_lines.append( '<START> ' + line + ' <END>' )  \n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( mar_lines ) \n",
        "tokenized_mar_lines = tokenizer.texts_to_sequences( mar_lines ) \n",
        "\n",
        "length_list = list()\n",
        "for token_seq in tokenized_mar_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_output_length = np.array( length_list ).max()\n",
        "print( 'Marathi max length is {}'.format( max_output_length ))\n",
        "\n",
        "padded_mar_lines = preprocessing.sequence.pad_sequences( tokenized_mar_lines , maxlen=max_output_length, padding='post' )\n",
        "decoder_input_data = np.array( padded_mar_lines )\n",
        "print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n",
        "\n",
        "mar_word_dict = tokenizer.word_index\n",
        "num_mar_tokens = len( mar_word_dict )+1\n",
        "print( 'Number of Marathi tokens = {}'.format( num_mar_tokens))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJTcSlygTQ_V"
      },
      "source": [
        "### 5) Preparing target data for the Decoder ( decoder_target_data ) \n",
        "\n",
        "We take a copy of `tokenized_mar_lines` and modify it like this.\n",
        "\n",
        "\n",
        "\n",
        "1.   We remove the `<start>` tag which we appended earlier. Hence, the word ( which is `<start>` in this case  ) will be removed.\n",
        "2.   Convert the `padded_mar_lines` ( ones which do not have `<start>` tag ) to one-hot vectors.\n",
        "\n",
        "For example :\n",
        "\n",
        "```\n",
        " [ '<start>' , 'hello' , 'world' , '<end>' ]\n",
        "\n",
        "```\n",
        "\n",
        "wil become \n",
        "\n",
        "```\n",
        " [ 'hello' , 'world' , '<end>' ]\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPCTmeL7qj3T"
      },
      "source": [
        "\n",
        "decoder_target_data = list()\n",
        "for token_seq in tokenized_mar_lines:\n",
        "    decoder_target_data.append( token_seq[ 1 : ] ) \n",
        "    \n",
        "padded_mar_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n",
        "onehot_mar_lines = utils.to_categorical( padded_mar_lines , num_mar_tokens )\n",
        "decoder_target_data = np.array( onehot_mar_lines )\n",
        "print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KS5gWlcpFT1"
      },
      "source": [
        "## Defining and Training the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_N71uykUPbe"
      },
      "source": [
        "### 1) Defining the Encoder-Decoder model\n",
        "The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n",
        "\n",
        "\n",
        "*   2 Input Layers : One for `encoder_input_data` and another for `decoder_input_data`.\n",
        "*   Embedding layer : For converting token vectors to fix sized dense vectors. **( Note :  Don't forget the `mask_zero=True` argument here )**\n",
        "*   LSTM layer : Provide access to Long-Short Term cells.\n",
        "\n",
        "Working : \n",
        "\n",
        "1.   The `encoder_input_data` comes in the Embedding layer (  `encoder_embedding` ). \n",
        "2.   The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( `h` and `c` which are `encoder_states` )\n",
        "3.   These states are set in the LSTM cell of the decoder.\n",
        "4.   The decoder_input_data comes in through the Embedding layer.\n",
        "5.   The Embeddings goes in LSTM cell ( which had the states ) to produce seqeunces.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqb4Bps1s_Lr",
        "cellView": "code"
      },
      "source": [
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( num_eng_tokens, 256 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 128 , return_state=True  )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( num_mar_tokens, 256 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 128 , return_state=True , return_sequences=True)\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( num_mar_tokens , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9g_8sR7WWf3"
      },
      "source": [
        "### 2) Training the model\n",
        "We train the model for a number of epochs with RMSprop optimizer and categorical crossentropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnd2H27qt4Hy",
        "cellView": "code"
      },
      "source": [
        "\n",
        "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=250, epochs=50 ) \n",
        "model.save( 'model.h5' ) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eeqv_vH5pMpb"
      },
      "source": [
        "## Inferencing on the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4PAtzGrk8pq"
      },
      "source": [
        "### 1) Defining inference models\n",
        "We create inference models which help in predicting translations.\n",
        "\n",
        "**Encoder inference model** : Takes the English sentence as input and outputs LSTM states ( `h` and `c` ).\n",
        "\n",
        "**Decoder inference model** : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the Marathi input seqeunces ( ones not having the `<start>` tag ). It will output the translations of the English sentence which we fed to the encoder model and its state values.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNhVkiZLvdTq"
      },
      "source": [
        "\n",
        "def make_inference_models():\n",
        "    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 128 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 128 ,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djEPrfJBmZE-"
      },
      "source": [
        "### 2) Making some translations\n",
        "\n",
        "\n",
        "1.   First, we take a English sequence and predict the state values using `enc_model`.\n",
        "2.   We set the state values in the decoder's LSTM.\n",
        "3.   Then, we generate a sequence which contains the `<start>` element.\n",
        "4.   We input this sequence in the `dec_model`.\n",
        "5.   We replace the `<start>` element with the element which was predicted by the `dec_model` and update the state values.\n",
        "6.   We carry out the above steps iteratively till we hit the `<end>` tag or the maximum sequence length.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_hrJcNP-mXb"
      },
      "source": [
        "\n",
        "def str_to_tokens( sentence : str ):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append( eng_word_dict[ word ] ) \n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mfco9WKukhS",
        "cellView": "both"
      },
      "source": [
        "\n",
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for epoch in range( encoder_input_data.shape[0] ):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter eng sentence : ' ) ) )\n",
        "    #states_values = enc_model.predict( encoder_input_data[ epoch ] )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = mar_word_dict['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in mar_word_dict.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation )\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}