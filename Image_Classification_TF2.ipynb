{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Classification_TF2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T1Pf4mbuVRI"
      },
      "source": [
        "\n",
        "#Image Classification From Scratch  With TensorFlow 2.0\n",
        "\n",
        "In this notebook, we'll be training a Convolutional Neural Network ( CNN ) without using Keras, all with raw TF 2.0 APIs!\n",
        "\n",
        "We'll declare weights, loss function and a optimizer for our CNN and train it on the Horses-Or-Humans dataset available on TensorFlow Datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcqgU2fXu-Yw"
      },
      "source": [
        "\n",
        "## 1) Installing TensorFlow 2.0\n",
        "\n",
        "First, if our Colab's default TensorFlow version is not 2.0 then we need to upgrade the TF package using `pip`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkcq3St-gZsd"
      },
      "source": [
        "\n",
        "!pip install --upgrade tensorflow\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOhEmurMvlO9"
      },
      "source": [
        "\n",
        "## 2) Importing packages and declaring useful variables\n",
        "\n",
        "We declare some useful variables like `batch_size`, `padding` for convolutional layers, the `learning_rate` and so on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYda818kgpz8"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "padding = \"SAME\"  #@param ['SAME', 'VALID' ]\n",
        "num_output_classes = 102  #@param {type: \"number\"}\n",
        "batch_size = 32  #@param {type: \"number\"}\n",
        "learning_rate = 0.001  #@param {type: \"number\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjsIAAxzxoii"
      },
      "source": [
        "\n",
        "## 3) Fetching the Dataset\n",
        "\n",
        "We fetch our dataset from TensorFlow Datasets which makes much of the data preprocessing easier ;-)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XglEBv0KjNF0"
      },
      "source": [
        "\n",
        "dataset_name = 'horses_or_humans'  #@param {type: \"string\"}\n",
        "\n",
        "dataset = tfds.load( name=dataset_name , split=tfds.Split.TRAIN )\n",
        "dataset = dataset.shuffle( 1024 ).batch( batch_size )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sZ3W9NCx8RB"
      },
      "source": [
        "\n",
        "## 4) Defining CNN operations\n",
        "\n",
        "For our CNN model, we mainly use three operations ( layers ).\n",
        "\n",
        "1. `conv2d` : Performs convolutions over the `inputs` matrix with kernels ( `filters` ) and `stride_size`. Also, it performs the Leaky ReLU activation function.\n",
        "\n",
        "2. `maxpool` : Performs max pooling over the `inputs`.\n",
        "\n",
        "3. `dense` : Dense layers for the CNN with dropout.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUwYW2IXg8hl"
      },
      "source": [
        "\n",
        "leaky_relu_alpha = 0.2 #@param {type: \"number\"}\n",
        "dropout_rate = 0.5 #@param {type: \"number\"}\n",
        "\n",
        "def conv2d( inputs , filters , stride_size ):\n",
        "    out = tf.nn.conv2d( inputs , filters , strides=[ 1 , stride_size , stride_size , 1 ] , padding=padding ) \n",
        "    return tf.nn.leaky_relu( out , alpha=leaky_relu_alpha ) \n",
        "\n",
        "def maxpool( inputs , pool_size , stride_size ):\n",
        "    return tf.nn.max_pool2d( inputs , ksize=[ 1 , pool_size , pool_size , 1 ] , padding='VALID' , strides=[ 1 , stride_size , stride_size , 1 ] )\n",
        "\n",
        "def dense( inputs , weights ):\n",
        "    x = tf.nn.leaky_relu( tf.matmul( inputs , weights ) , alpha=leaky_relu_alpha )\n",
        "    return tf.nn.dropout( x , rate=dropout_rate )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XNdAsWky7eg"
      },
      "source": [
        "\n",
        "## 5) Initializing CNN weights\n",
        "\n",
        "We initialize the weights for our CNN. The shapes need to calculated but the `tf.nn.conv2d` expects the filters to have a shape of `[ kernel_size , kernel_size , in_dims , out_dims ]`.\n",
        "\n",
        "We use the `glorot_uniform` initializer for our weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXSI7DnehJhE"
      },
      "source": [
        "\n",
        "output_classes = 3\n",
        "initializer = tf.initializers.glorot_uniform()\n",
        "def get_weight( shape , name ):\n",
        "    return tf.Variable( initializer( shape ) , name=name , trainable=True , dtype=tf.float32 )\n",
        "\n",
        "shapes = [\n",
        "    [ 3 , 3 , 3 , 16 ] , \n",
        "    [ 3 , 3 , 16 , 16 ] , \n",
        "    [ 3 , 3 , 16 , 32 ] , \n",
        "    [ 3 , 3 , 32 , 32 ] ,\n",
        "    [ 3 , 3 , 32 , 64 ] , \n",
        "    [ 3 , 3 , 64 , 64 ] ,\n",
        "    [ 3 , 3 , 64 , 128 ] , \n",
        "    [ 3 , 3 , 128 , 128 ] ,\n",
        "    [ 3 , 3 , 128 , 256 ] , \n",
        "    [ 3 , 3 , 256 , 256 ] ,\n",
        "    [ 3 , 3 , 256 , 512 ] , \n",
        "    [ 3 , 3 , 512 , 512 ] ,\n",
        "    [ 8192 , 3600 ] , \n",
        "    [ 3600 , 2400 ] ,\n",
        "    [ 2400 , 1600 ] , \n",
        "    [ 1600 , 800 ] ,\n",
        "    [ 800 , 64 ] ,\n",
        "    [ 64 , output_classes ] ,\n",
        "]\n",
        "\n",
        "weights = []\n",
        "for i in range( len( shapes ) ):\n",
        "    weights.append( get_weight( shapes[ i ] , 'weight{}'.format( i ) ) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsCz6ONvziKX"
      },
      "source": [
        "\n",
        "## 6) Assembling the operations\n",
        "\n",
        "We put together all the CNN ops we defined earlier into a final model which we will use for training finally.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py8Ggzs2hPqO"
      },
      "source": [
        "\n",
        "def model( x ) :\n",
        "    x = tf.cast( x , dtype=tf.float32 )\n",
        "    c1 = conv2d( x , weights[ 0 ] , stride_size=1 ) \n",
        "    c1 = conv2d( c1 , weights[ 1 ] , stride_size=1 ) \n",
        "    p1 = maxpool( c1 , pool_size=2 , stride_size=2 )\n",
        "    \n",
        "    c2 = conv2d( p1 , weights[ 2 ] , stride_size=1 )\n",
        "    c2 = conv2d( c2 , weights[ 3 ] , stride_size=1 ) \n",
        "    p2 = maxpool( c2 , pool_size=2 , stride_size=2 )\n",
        "    \n",
        "    c3 = conv2d( p2 , weights[ 4 ] , stride_size=1 ) \n",
        "    c3 = conv2d( c3 , weights[ 5 ] , stride_size=1 ) \n",
        "    p3 = maxpool( c3 , pool_size=2 , stride_size=2 )\n",
        "    \n",
        "    c4 = conv2d( p3 , weights[ 6 ] , stride_size=1 )\n",
        "    c4 = conv2d( c4 , weights[ 7 ] , stride_size=1 )\n",
        "    p4 = maxpool( c4 , pool_size=2 , stride_size=2 )\n",
        "\n",
        "    c5 = conv2d( p4 , weights[ 8 ] , stride_size=1 )\n",
        "    c5 = conv2d( c5 , weights[ 9 ] , stride_size=1 )\n",
        "    p5 = maxpool( c5 , pool_size=2 , stride_size=2 )\n",
        "\n",
        "    c6 = conv2d( p5 , weights[ 10 ] , stride_size=1 )\n",
        "    c6 = conv2d( c6 , weights[ 11 ] , stride_size=1 )\n",
        "    p6 = maxpool( c6 , pool_size=2 , stride_size=2 )\n",
        "\n",
        "    flatten = tf.reshape( p6 , shape=( tf.shape( p6 )[0] , -1 ))\n",
        "\n",
        "    d1 = dense( flatten , weights[ 12 ] )\n",
        "    d2 = dense( d1 , weights[ 13 ] )\n",
        "    d3 = dense( d2 , weights[ 14 ] )\n",
        "    d4 = dense( d3 , weights[ 15 ] )\n",
        "    d5 = dense( d4 , weights[ 16 ] )\n",
        "    logits = tf.matmul( d5 , weights[ 17 ] )\n",
        "\n",
        "    return tf.nn.softmax( logits )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P-L8iQWz29J"
      },
      "source": [
        "\n",
        "## 7) Defining the loss function and optimization using `tf.GradientTape`\n",
        "\n",
        "We use `tf.losses.categorical_crossentropy` as our loss function. Now comes `tf.GradientTape`, an automatic differentiation engine which records all the derivatives within its scope.\n",
        "\n",
        "The `train_step` function takes in a batch of data, calculates the loss and gradients and then with `optimizer.apply_gradients`, we update the `weights`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U61C5trri_xu"
      },
      "source": [
        "\n",
        "def loss( pred , target ):\n",
        "    return tf.losses.categorical_crossentropy( target , pred )\n",
        "\n",
        "optimizer = tf.optimizers.Adam( learning_rate )\n",
        "\n",
        "def train_step( model, inputs , outputs ):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = loss( model( inputs ), outputs)\n",
        "    grads = tape.gradient( current_loss , weights )\n",
        "    optimizer.apply_gradients( zip( grads , weights ) )\n",
        "    print( tf.reduce_mean( current_loss ) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8CUgMw70V9P"
      },
      "source": [
        "\n",
        "## 8) Final Training\n",
        "\n",
        "We train our model for a specific number of epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL42kBKgk2U2"
      },
      "source": [
        "\n",
        "num_epochs = 256 #@param {type: \"number\"}\n",
        "\n",
        "for e in range( num_epochs ):\n",
        "    for features in dataset:\n",
        "        image , label = features[ 'image' ] , features[ 'label' ]\n",
        "        train_step( model , image , tf.one_hot( label , depth=3 ) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSdM0ti0l-lO"
      },
      "source": [
        "\n",
        "That's All. You can now export the graph or play around with more layers or fine-tuning the hyperparameters.\n"
      ]
    }
  ]
}