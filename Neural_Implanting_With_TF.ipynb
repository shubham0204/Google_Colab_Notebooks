{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "Neural_Implanting_With_TF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXjElGKzyiey"
      },
      "source": [
        "\n",
        "# **Constructing Missing Parts In An Image Using Auto Encoders**\n",
        "\n",
        "\n",
        "[<img src=\"https://github.com/shubham0204/Privacy_Policy_Texts/blob/master/notebook_button.png?raw=true\" width=\"210\" height=\"40\" align=\"center\">](https://medium.com/@equipintelligence/neural-implanting-with-autoencoders-and-tensorflow-9c2c7b532198)\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, we'll learn to make a model which can construct missing parts of an image. Given an image ( which has some part of it missing ), our model will try to construct that missing part with the help of AutoEncoders.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJP6HFVXY3WH"
      },
      "source": [
        "\n",
        "## 1) Downloading the images\n",
        "\n",
        "First, we'll download some images of mountains which I've hosted on GitHub. This images are from the [Intel Image Classification](https://www.kaggle.com/puneet6060/intel-image-classification) dataset by [Puneet Bansal](https://www.kaggle.com/puneet6060) on [Kaggle](https://www.kaggle.com/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efogD_Hprcax"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "from tensorflow.keras.layers import Conv2D , Conv2DTranspose , MaxPooling2D , UpSampling2D , Input , LeakyReLU , Concatenate\n",
        "\n",
        "!wget https://github.com/shubham0204/Dataset_Archives/blob/master/mountain_images.zip?raw=true -O images.zip\n",
        "!unzip images.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd6MEZYFZvDh"
      },
      "source": [
        "\n",
        "## 2) Preparing the input data\n",
        "\n",
        "Our job is to construct the missing parts in an image. So, as inputs to our model, we have to randomly take a square part ( whose side length is `window_size` ) from an image and replace it with a zero array of the same shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBBGNGM-rcbR"
      },
      "source": [
        "\n",
        "x = []\n",
        "y = []\n",
        "input_size = ( 228 , 228 , 3 )\n",
        "\n",
        "# Take out a square region of side 50 px.\n",
        "window_size = 50\n",
        "\n",
        "# Store the original images as target images.\n",
        "for name in os.listdir( 'mountain_images/' ):\n",
        "    image = Image.open( 'mountain_images/{}'.format( name ) ).resize( input_size[0:2] )\n",
        "    image = np.asarray( image ).astype( np.uint8 )\n",
        "    y.append( image )\n",
        "\n",
        "for name in os.listdir( 'mountain_images/' ):\n",
        "    image = Image.open( 'mountain_images/{}'.format( name ) ).resize( input_size[0:2] )\n",
        "    image = np.asarray( image ).astype( np.uint8 )\n",
        "    # Generate random X and Y coordinates within the image bounds.\n",
        "    px , py = random.randint( 0 , input_size[0] - window_size ) , random.randint( 0 , input_size[0] - window_size )\n",
        "    # Take that part of the image and replace it with a zero array. This makes the \"missing\" part of the image.\n",
        "    image[ px : px + window_size , py : py + window_size , 0:3 ] = np.zeros( ( window_size , window_size , 3 ) )\n",
        "    # Append it to an array\n",
        "    x.append( image )\n",
        "    \n",
        "#  Normalize the images\n",
        "x = np.array( x ) / 255\n",
        "y = np.array( y ) / 255\n",
        "\n",
        "print( x.shape )\n",
        "print( y.shape )\n",
        "\n",
        "# Train test split\n",
        "x_train, x_test, y_train, y_test = train_test_split( x , y , test_size=0.2 )\n",
        "\n",
        "# Plot an image to see what happened!\n",
        "plt.imshow( x_test[0] )\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mccqs3Y1bWK7"
      },
      "source": [
        "\n",
        "## 3) The Auto Encoder Model\n",
        "\n",
        "We'll create a simple Convolutional Auto Encoder, which has skip connections ( similar to a UNet ), whose input and output shapes are the same. We'll use `tf.keras.losses.mean_squared_error` and `tf.keras.optimizers.Adam`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2vq56xYBQml"
      },
      "source": [
        "\n",
        "alpha = 0.2\n",
        "\n",
        "inputs = Input( shape=input_size )\n",
        "conv1 = Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1 )( inputs )\n",
        "relu1 = LeakyReLU( alpha )( conv1 )\n",
        "conv2 = Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1 )( relu1 )\n",
        "relu2 = LeakyReLU( alpha )( conv2 )\n",
        "maxpool1 = MaxPooling2D()( relu2 )\n",
        "\n",
        "conv3 = Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1 )( maxpool1 )\n",
        "relu3 = LeakyReLU( alpha )( conv3 )\n",
        "conv4 = Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1 )( relu3 )\n",
        "relu4 = LeakyReLU( alpha )( conv4 )\n",
        "maxpool2 = MaxPooling2D()( relu4 )\n",
        "\n",
        "conv5 = Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 )( maxpool2 )\n",
        "relu5 = LeakyReLU( alpha )( conv5 )\n",
        "conv6 = Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 )( relu5 )\n",
        "relu6 = LeakyReLU( alpha )( conv6 )\n",
        "maxpool3 = MaxPooling2D()( relu6 )\n",
        "\n",
        "conv7 = Conv2D( 256 , kernel_size=( 1 , 1 ) , strides=1 )( maxpool3 )\n",
        "relu7 = LeakyReLU( alpha )( conv7 )\n",
        "conv8 = Conv2D( 256 , kernel_size=( 1 , 1 ) , strides=1 )( relu7 )\n",
        "relu8 = LeakyReLU( alpha )( conv8 )\n",
        "\n",
        "upsample1 = UpSampling2D()( relu8 )\n",
        "concat1 = Concatenate()([ upsample1 , conv6 ])\n",
        "convtranspose1 = Conv2DTranspose( 128 , kernel_size=( 3 , 3 ) , strides=1)( concat1 )\n",
        "relu9 = LeakyReLU( alpha )( convtranspose1 )\n",
        "convtranspose2 = Conv2DTranspose( 128 , kernel_size=( 3 , 3 ) , strides=1  )( relu9 )\n",
        "relu10 = LeakyReLU( alpha )( convtranspose2 )\n",
        "\n",
        "upsample2 = UpSampling2D()( relu10 )\n",
        "concat2 = Concatenate()([ upsample2 , conv4 ])\n",
        "convtranspose3 = Conv2DTranspose( 64 , kernel_size=( 3 , 3 ) , strides=1)( concat2 )\n",
        "relu11 = LeakyReLU( alpha )( convtranspose3 )\n",
        "convtranspose4 = Conv2DTranspose( 64 , kernel_size=( 3 , 3 ) , strides=1 )( relu11 )\n",
        "relu12 = LeakyReLU( alpha )( convtranspose4 )\n",
        "\n",
        "upsample3 = UpSampling2D()( relu12 )\n",
        "concat3 = Concatenate()([ upsample3 , conv2 ])\n",
        "convtranspose5 = Conv2DTranspose( 32 , kernel_size=( 3 , 3 ) , strides=1)( concat3 )\n",
        "relu13 = LeakyReLU( alpha )( convtranspose5 )\n",
        "convtranspose6 = Conv2DTranspose( 3 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( relu13 )\n",
        "\n",
        "model = tf.keras.models.Model( inputs , convtranspose6 )\n",
        "model.compile( loss='mse' , optimizer='adam' , metrics=[ 'mse' ] )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIhEb8qsb0N7"
      },
      "source": [
        "\n",
        "## 4) Training the model\n",
        "\n",
        "We'll train the model with a batch size of 50. To visualize the training process, you may use [TensorBoard](https://www.tensorflow.org/tensorboard). See [Using TensorBoard in Notebooks](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enxzZm0urcba"
      },
      "source": [
        "\n",
        "model.fit( x_train , y_train , epochs=30 , batch_size=25 , validation_data=( x_test , y_test ) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foHLpIHPcaTp"
      },
      "source": [
        "\n",
        "## 5) Plotting the Results\n",
        "\n",
        "We'll make some predictions on test images and plot the results. You'll probably see a square in the image where our model has constructed the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyf97fRprcbk"
      },
      "source": [
        "\n",
        "fig = plt.figure(figsize=( 50 , 50 ))\n",
        "for i in range( 1 , 6 ):\n",
        "    pred = model.predict( x_test[ i : i+1 ] ) * 255\n",
        "    fig.add_subplot( 1 , 10 , i )\n",
        "    plt.imshow( pred[0].astype( np.uint8 )  )\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}