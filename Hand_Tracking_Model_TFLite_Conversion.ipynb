{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hand_Tracking_Model_TFLite_Conversion.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icHN0TBztjsy"
      },
      "source": [
        "\n",
        "# Hand Tracking Model Conversion to TFLite\n",
        "\n",
        "Last Updated: 09/09/2021\n",
        "\n",
        "> [**Open in Google Colab**](https://colab.research.google.com/github/shubham0204/Google_Colab_Notebooks/blob/main/Hand_Tracking_Model_TFLite_Conversion.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRnFc5txtvHx"
      },
      "source": [
        "\n",
        "## 1) Installing TensorFlow 1.x\n",
        "\n",
        "As we know that Colab is shipped with TensorFlow 2.x by default ( you may check this if you need ), we need to install TensorFlow 1.15.0, the last version of TensorFlow 1.x family.\n",
        "\n",
        "This is important because the project [victordibia/handtracking](https://github.com/victordibia/handtracking) seems to be created with TensorFlow 1.x.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbW68wYoqlLM"
      },
      "source": [
        "\n",
        "!pip install tensorflow==1.15.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzZm5mLEupLb"
      },
      "source": [
        "\n",
        "## 2) Cloning TF Object Detection API\n",
        "\n",
        "In order to convert the checkpoint files to the TFLite format, we'll need the [TensorFlow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection), as it contains various scripts to perform the conversion.\n",
        "\n",
        "\n",
        "There's no separate API, its just a part of the [tensorflow/models](https://github.com/tensorflow/models) repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xevRPNrejvAq"
      },
      "source": [
        "\n",
        "!git clone --depth 1 https://github.com/tensorflow/models\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xhzpnOWQZOX"
      },
      "source": [
        "\n",
        "We also clone Victor Dibia's [handtracking](https://github.com/victordibia/handtracking) repository in order to use the trained checkpoints.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z2Uxfr6QYee"
      },
      "source": [
        "\n",
        "!git clone https://github.com/victordibia/handtracking\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwduW4zsQt4s"
      },
      "source": [
        "\n",
        "## 3) Installing TF Object Detection API\n",
        "\n",
        "Once we have cloned the TF OD API, we're ready to install the packages required by the API. This is easy to do in Colab, as we only need to run some commands, which can also be seen here.\n",
        "\n",
        "You can go through the installation steps [here](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1au1bRM-kEMG"
      },
      "source": [
        "\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf1/setup.py .\n",
        "python -m pip install .\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXQkl81ARelZ"
      },
      "source": [
        "\n",
        "## 4) Exporting the trained checkpoints to a TFLite-compatible graph\n",
        "\n",
        "Using the `export_tflite_ssd_graph.py` script in the `models/research/object_detection` directory, we'll convert the trained checkpoints, present in `handtracking/model-checkpoint` directory, to a TFLite-compatible graph.\n",
        "\n",
        "The script takes in three arguments,\n",
        "\n",
        "1. `pipeline_config_path` : This is the path to the `.config` file of your trained model.\n",
        "\n",
        "2. `trained_checkpoint_prefix` : Prefix of trained checkpoints we need to convert .\n",
        "\n",
        "3. `max_detections` : Number of detections the model will make. **This is important**, as it determines the output shapes of your model. We'll come to this in step 6.\n",
        "\n",
        "The outputs of this script are two files, `tflite_graph.pb` and `tflite_graph.pbtxt` in the `outputs` directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI2MvX9wkV3X"
      },
      "source": [
        "\n",
        "!python /content/models/research/object_detection/export_tflite_ssd_graph.py \\\n",
        "    --pipeline_config_path /content/handtracking/model-checkpoint/ssdlitemobilenetv2/data_ssdlite.config \\\n",
        "    --trained_checkpoint_prefix /content/handtracking/model-checkpoint/ssdlitemobilenetv2/out_model.ckpt-19040 \\\n",
        "    --output_directory outputs\\\n",
        "    --max_detections 10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsyKoRrIUpsC"
      },
      "source": [
        "\n",
        "## 5) Convert the TFLite-compatible graph to a TFLite model ( .tflite )\n",
        "\n",
        "Using the utility tool `tflite_convert`, we convert the TFLite-compatible graph to a TFLite ( .tflite ) model. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAswx8jhloEr"
      },
      "source": [
        "\n",
        "!tflite_convert \\\n",
        "--graph_def_file=/content/outputs/tflite_graph.pb \\\n",
        "--output_file=/content/outputs/model.tflite \\\n",
        "--output_format=TFLITE \\\n",
        "--input_arrays=normalized_input_image_tensor \\\n",
        "--input_shapes=1,300,300,3 \\\n",
        "--inference_type=FLOAT \\\n",
        "--output_arrays=\"TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3\" \\\n",
        "--allow_custom_ops\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xg0e4xbVGlW"
      },
      "source": [
        "\n",
        "## 6) Inspect the input/output shapes of the TFLite model\n",
        "\n",
        "Using the `tf.lite.Interpreter` class, we'll check the input and output shapes ( and dtypes ) of our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKJh4LOOsgYN"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import pprint\n",
        "\n",
        "interpreter = tf.lite.Interpreter( '/content/outputs/model.tflite' )\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "pprint.pprint( interpreter.get_input_details())\n",
        "pprint.pprint( interpreter.get_output_details() )\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}