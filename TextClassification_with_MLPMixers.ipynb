{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sustainable-johnson",
   "metadata": {
    "papermill": {
     "duration": 0.021333,
     "end_time": "2021-06-10T12:19:14.812097",
     "exception": false,
     "start_time": "2021-06-10T12:19:14.790764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<center><h1>üê§ <b><u>Tweet Classification with MLP-Mixers ( TF-Keras )</u></b></h1></center>\n",
    "\n",
    "![mlp_mixer_diagram](https://github.com/shubham0204/Google_Colab_Notebooks/blob/main/data/mlp_mixer_diagram.PNG?raw=true)\n",
    "\n",
    "> Image Source: [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601)\n",
    "\n",
    "\n",
    "Entering the **\"Natural Language Processing with Disaster Tweets\"**, our goal is to design a ML model which can efficiently classify tweets into two categories, the ones which refer ( or describe ) to a üò± disaster and others which don't contain any such context üòÅ.\n",
    "\n",
    "> In this notebook, you'll learn how to implement an MLP-Mixer architecture for classifying with TensorFlow Keras.\n",
    "\n",
    "Here's the list of all things we'll do ( and enjoy! ) in this notebook,\n",
    "\n",
    "* First, we process the raw texts in order to üßπ eliminate unwanted characters and symbols.\n",
    "* We implement our MLP-Mixer model with TensorFlow and train it on the processed data.\n",
    "* Evaluate the model and produce the `submission.csv` file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-booking",
   "metadata": {
    "papermill": {
     "duration": 0.019507,
     "end_time": "2021-06-10T12:19:14.851553",
     "exception": false,
     "start_time": "2021-06-10T12:19:14.832046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## 1. üõ† Processing the text data\n",
    "\n",
    "### a. ‚úÇÔ∏è **Reading and truncating the CSV data**\n",
    "\n",
    "The data provided to us is in the CSV format. First, we parse the CSV file using `pandas.read_csv` which transform it into a `DataFrame` object, which eases data handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "frequent-xerox",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T12:19:14.895296Z",
     "iopub.status.busy": "2021-06-10T12:19:14.894053Z",
     "iopub.status.idle": "2021-06-10T12:19:22.299819Z",
     "shell.execute_reply": "2021-06-10T12:19:22.298913Z",
     "shell.execute_reply.started": "2021-06-10T12:02:09.867368Z"
    },
    "papermill": {
     "duration": 7.428861,
     "end_time": "2021-06-10T12:19:22.300018",
     "exception": false,
     "start_time": "2021-06-10T12:19:14.871157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Importing the required packages.\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-usage",
   "metadata": {
    "papermill": {
     "duration": 0.019466,
     "end_time": "2021-06-10T12:19:22.339818",
     "exception": false,
     "start_time": "2021-06-10T12:19:22.320352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "As seen in the output of the code cell below, the CSV data contains 5 columns. Considering our problem, we'll truncate `df` and use `text` and `target` columns. The reason behind eliminating columns `keyword` and `location` is,\n",
    "\n",
    "* As we classifying tweets, the `location` üè† of a tweet is insignificant. The column also contains several null entries ( interpreted as `NaN` ).\n",
    "* The `keyword` column could be used as a *metadata* to the existing tweet. But we drop its use as it contains several `NaN` values. Even if we try to drop these rows containing `NaN` using `pandas.dropNa()`, we'll be left with insufficient data for training our model. ( Neural networks require larger amounts of data for training )\n",
    "\n",
    "Also, we store the entries of these columns in NumPy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "double-witch",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T12:19:22.384699Z",
     "iopub.status.busy": "2021-06-10T12:19:22.383876Z",
     "iopub.status.idle": "2021-06-10T12:19:22.488141Z",
     "shell.execute_reply": "2021-06-10T12:19:22.487410Z",
     "shell.execute_reply.started": "2021-06-10T12:02:16.073116Z"
    },
    "papermill": {
     "duration": 0.128948,
     "end_time": "2021-06-10T12:19:22.488292",
     "exception": false,
     "start_time": "2021-06-10T12:19:22.359344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Read the CSV file using Pandas\n",
    "df = pd.read_csv( '../input/nlp-getting-started/train.csv' )\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lyric-patient",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T12:19:22.535338Z",
     "iopub.status.busy": "2021-06-10T12:19:22.534161Z",
     "iopub.status.idle": "2021-06-10T12:19:22.564799Z",
     "shell.execute_reply": "2021-06-10T12:19:22.565304Z",
     "shell.execute_reply.started": "2021-06-10T12:02:18.496729Z"
    },
    "papermill": {
     "duration": 0.056745,
     "end_time": "2021-06-10T12:19:22.565472",
     "exception": false,
     "start_time": "2021-06-10T12:19:22.508727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target\n",
      "0  Our Deeds are the Reason of this #earthquake M...       1\n",
      "1             Forest fire near La Ronge Sask. Canada       1\n",
      "2  All residents asked to 'shelter in place' are ...       1\n",
      "3  13,000 people receive #wildfires evacuation or...       1\n",
      "4  Just got sent this photo from Ruby #Alaska as ...       1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Truncate the dataFrame and use only `text` and `target` columns.\n",
    "df = pd.read_csv( '../input/nlp-getting-started/train.csv' , usecols=[ 'text' , 'target' ] )\n",
    "print( df.head() )\n",
    "\n",
    "# Store the entries in the above mentioned columns in NumPy arrays.\n",
    "raw_texts = df[ 'text' ].values\n",
    "raw_labels = df[ 'target' ].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-attraction",
   "metadata": {
    "papermill": {
     "duration": 0.020024,
     "end_time": "2021-06-10T12:19:22.605724",
     "exception": false,
     "start_time": "2021-06-10T12:19:22.585700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "In the output of the code cell below, you'll observe that we have unequal samples belonging to both the classes. This may affect our model's performance on any of the classes. Hence, we compute the class weights using `sklearn.utils.class_weight.compute_class_weight` which will be used for weighing the losses for each of the classes.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "single-announcement",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T12:19:22.656145Z",
     "iopub.status.busy": "2021-06-10T12:19:22.655282Z",
     "iopub.status.idle": "2021-06-10T12:19:22.666897Z",
     "shell.execute_reply": "2021-06-10T12:19:22.667360Z",
     "shell.execute_reply.started": "2021-06-10T12:02:20.571582Z"
    },
    "papermill": {
     "duration": 0.041527,
     "end_time": "2021-06-10T12:19:22.667541",
     "exception": false,
     "start_time": "2021-06-10T12:19:22.626014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass classes=[0 1], y=[1 1 1 ... 1 1 1] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the no. of samples for each class.\n",
    "print( df[ 'target' ].value_counts() )\n",
    "\n",
    "# Compute the class weights\n",
    "# See this answer -> https://datascience.stackexchange.com/a/69302/68023\n",
    "class_weights = dict( zip( np.unique( raw_labels ), sklearn.utils.class_weight.compute_class_weight( 'balanced', np.unique( raw_labels ), raw_labels ))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-recording",
   "metadata": {
    "papermill": {
     "duration": 0.020474,
     "end_time": "2021-06-10T12:19:22.708949",
     "exception": false,
     "start_time": "2021-06-10T12:19:22.688475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### b. üßπ **Cleaning and tokenizing the textual data**\n",
    "\n",
    "![tokenization](https://www.kdnuggets.com/wp-content/uploads/text-tokens-tokenization-manning.jpg)\n",
    "\n",
    "> Image Source: [Tokenization and Text Data Preparation - KDNuggets](https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html)\n",
    "\n",
    "As you might have observed, the text ( of the tweets ) contains hashtags and URLs to images attached with the tweet. These are unnecessary features and hence we use regular expressions to filter each tweet. We perform a two-step filtration process:\n",
    "\n",
    "1. Convert the words in the tweet to lowercase. Also, remove all non-alphabet characters from the text, like numbers, punctuations. Refer to this [StackOverflow answer](https://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string).\n",
    "\n",
    "2. Remove hyperlinks from the tweet. Refer to this [StackOverflow answer](https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python/11332580)\n",
    "\n",
    "3. Remove üòÄ emojis from the tweet. Refer to this [StackOverflow answer](https://stackoverflow.com/a/33417311/10878733)\n",
    "\n",
    "Finally, we tokenize all the sentences and store these tokens in `processed_tokens`. After eliminating all duplicate tokens, we assign an index to each token and create two dictionaries which map each token to its index and vice-versa. See `word_to_index` and `index_to_word`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "starting-indonesian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T12:19:22.765221Z",
     "iopub.status.busy": "2021-06-10T12:19:22.764471Z",
     "iopub.status.idle": "2021-06-10T12:19:35.591385Z",
     "shell.execute_reply": "2021-06-10T12:19:35.590732Z",
     "shell.execute_reply.started": "2021-06-10T12:02:23.418576Z"
    },
    "papermill": {
     "duration": 12.861557,
     "end_time": "2021-06-10T12:19:35.591528",
     "exception": false,
     "start_time": "2021-06-10T12:19:22.729971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Regex to remove non-alphabet characters\\\n",
    "r1 = re.compile( '[^a-zA-Z ]' )\n",
    "\n",
    "# Regex to remove hyperlinks ( \"https://...\" )\n",
    "r2 = re.compile( 'http://\\S+|https://\\S+' )\n",
    "\n",
    "# Regex to extract words starting with #\n",
    "r3 = re.compile( '#(\\w+)' )\n",
    "\n",
    "# Remove non-alphabet char from given sentence. See https://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string\n",
    "def remove_non_alphabet_char( sent ):\n",
    "    return r1.sub( '' , sent )\n",
    "\n",
    "# Remove hyperlinks from given sentence. See https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python/11332580\n",
    "def remove_links( sent ):\n",
    "    return r2.sub( '' , sent )\n",
    "\n",
    "# Remove emojis. See https://stackoverflow.com/a/33417311/10878733\n",
    "def remove_emojis( sent ):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub( r'',sent )\n",
    "    \n",
    "\n",
    "# Clean the given sentence ( using the two methods above ) and tokenize it.\n",
    "# Also, remove stop words from the sentence.\n",
    "def process_sent( sent ):\n",
    "    sent = text.lower()\n",
    "    sent = remove_links( sent )\n",
    "    sent = remove_non_alphabet_char( sent )\n",
    "    sent = remove_emojis( sent )\n",
    "    tokens = sent.split()\n",
    "    tokens = [ token.strip() for token in tokens if token not in stopwords.words( 'english' ) ]\n",
    "    return tokens\n",
    "    \n",
    "# Collect tokens and tokenized sentences in two arrays.\n",
    "processed_tokens = []\n",
    "tokenized_sentences = []\n",
    "for text in raw_texts:\n",
    "    tokens = process_sent( text )\n",
    "    processed_tokens += tokens\n",
    "    tokenized_sentences.append( tokens )\n",
    "\n",
    "# Get unique tokens\n",
    "unique_tokens = list( set( processed_tokens ) )\n",
    "unique_tokens = np.array( unique_tokens )\n",
    "\n",
    "# Compute vocabulary size ( will be used for the Embedding layer )\n",
    "vocab_size = len( unique_tokens )\n",
    "# Create an array of indexed starting from 1 to vocab_size + 1 \n",
    "# For ex. [ 1 , 2 , 3 , ... , vocab_size ]\n",
    "indices = np.arange( 1 , vocab_size + 1 )\n",
    "\n",
    "# Zip unique_tokens and indices to create a dict with elements ( index , token ) where index has dtype=int and\n",
    "# token has dtype=str\n",
    "# This dict maps every index to its corresponding token.\n",
    "int_to_word = dict( zip( indices , unique_tokens ) )\n",
    "\n",
    "# This dict maps every token to its corresponding index.\n",
    "word_to_int = dict( zip( unique_tokens , indices ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-disease",
   "metadata": {
    "papermill": {
     "duration": 0.020187,
     "end_time": "2021-06-10T12:19:35.632314",
     "exception": false,
     "start_time": "2021-06-10T12:19:35.612127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Our next step is to transform each tokenized sentence into a integer sequence. This conversion is performed by the `sent_to_int_seq()` method which takes in tokenized sentence like `[ 'apple' , 'orangle' ]` and maps it to a integer sequence ( consisting of tokens' indices retreived from `word_to_int` ) like `[ 1023 , 1102 ]`.\n",
    "\n",
    "Also, we compute the maximum length of these sequences in order to perform zero padding using `pad_sequence()`\n",
    "\n",
    "```\n",
    "maxlen = max( [ len( arr ) for arr in tokenized_sentences ] )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "horizontal-upper",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T12:19:38.444259Z",
     "iopub.status.busy": "2021-06-10T12:19:38.443483Z",
     "iopub.status.idle": "2021-06-10T12:19:50.485380Z",
     "shell.execute_reply": "2021-06-10T12:19:50.484842Z",
     "shell.execute_reply.started": "2021-06-10T12:02:36.227257Z"
    },
    "papermill": {
     "duration": 14.832686,
     "end_time": "2021-06-10T12:19:50.485529",
     "exception": false,
     "start_time": "2021-06-10T12:19:35.652843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length for input sequences : 23\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transform the given tokenized sentence to an integer sequence\n",
    "# For example, [ 'apple' , 'orange' ] --> [ 1023 , 1102]\n",
    "def sent_to_int_seq( sent ):\n",
    "    int_seq = [ word_to_int[ token ] if token in unique_tokens else 0 for token in sent ]\n",
    "    return int_seq\n",
    "\n",
    "# Pad the given integer sequence with zeros ( from the end of the sequences )\n",
    "# For example, if maxlen=5,\n",
    "# [ 56 , 78 ] -> [ 56 , 78 , 0 , 0 , 0 ]\n",
    "# [ 34 , 56 , 78 , 23 , 13 , 12 ] -> [ 34 , 56 , 78 , 23 , 13 ]\n",
    "def pad_sequence( seq , maxlen ):\n",
    "    out = np.zeros( shape=( maxlen , ) )\n",
    "    out[ 0 : len( seq ) ] = seq\n",
    "    return out\n",
    "\n",
    "# Compute the max length of the tokenized sentences.\n",
    "# Will be used for padding the sequences.\n",
    "maxlen = max( [ len( arr ) for arr in tokenized_sentences ] )\n",
    "print( f'Max Length for input sequences : {maxlen}')\n",
    "\n",
    "# Convert tokenized_sentences to integer sequences\n",
    "# Finally pad the integer sequence and store it in an array.\n",
    "padded_sentences = []\n",
    "for sent in tokenized_sentences:\n",
    "    padded_sentences.append( pad_sequence( sent_to_int_seq( sent ) , maxlen ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-sheffield",
   "metadata": {
    "papermill": {
     "duration": 0.020652,
     "end_time": "2021-06-10T12:19:50.527334",
     "exception": false,
     "start_time": "2021-06-10T12:19:50.506682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "We have cleaned the data and now we are left with integer sequences of shape `( num_samples , maxlen )` and their corresponding labels of shape `( num_samples , 1 )`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "greenhouse-wales",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T12:19:50.576036Z",
     "iopub.status.busy": "2021-06-10T12:19:50.575056Z",
     "iopub.status.idle": "2021-06-10T12:19:50.587126Z",
     "shell.execute_reply": "2021-06-10T12:19:50.586509Z",
     "shell.execute_reply.started": "2021-06-10T12:02:50.730357Z"
    },
    "papermill": {
     "duration": 0.038814,
     "end_time": "2021-06-10T12:19:50.587310",
     "exception": false,
     "start_time": "2021-06-10T12:19:50.548496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 23)\n",
      "(7613, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert list to ndarray\n",
    "x = np.array( padded_sentences )\n",
    "print( x.shape )\n",
    "\n",
    "# Reshape raw_labels from shape ( num_labels , ) to ( num_labels , 1 )\n",
    "#y = raw_labels.reshape( -1 , 1) \n",
    "y = tf.keras.utils.to_categorical( raw_labels , num_classes=2 )\n",
    "print( y.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-laser",
   "metadata": {
    "papermill": {
     "duration": 0.021846,
     "end_time": "2021-06-10T12:19:50.632021",
     "exception": false,
     "start_time": "2021-06-10T12:19:50.610175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "We split our data into training and testing datasets using `sklearn.model_selection.train_test_split()` using 20% of the data for evaluating our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "current-burke",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T12:19:50.683114Z",
     "iopub.status.busy": "2021-06-10T12:19:50.682338Z",
     "iopub.status.idle": "2021-06-10T12:19:50.689402Z",
     "shell.execute_reply": "2021-06-10T12:19:50.688774Z",
     "shell.execute_reply.started": "2021-06-10T12:02:50.746312Z"
    },
    "papermill": {
     "duration": 0.03518,
     "end_time": "2021-06-10T12:19:50.689595",
     "exception": false,
     "start_time": "2021-06-10T12:19:50.654415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6090, 23)\n",
      "(6090, 2)\n",
      "(1523, 23)\n",
      "(1523, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into training and testing datasets.\n",
    "train_x , test_x , train_y , test_y = sklearn.model_selection.train_test_split( x , y , test_size=0.2 )\n",
    "print( train_x.shape )\n",
    "print( train_y.shape )\n",
    "print( test_x.shape )\n",
    "print( test_y.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-deployment",
   "metadata": {
    "papermill": {
     "duration": 0.022022,
     "end_time": "2021-06-10T12:19:50.734311",
     "exception": false,
     "start_time": "2021-06-10T12:19:50.712289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "We have completed the data preprocessing. We'll now discuss more on the training of our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-stephen",
   "metadata": {
    "papermill": {
     "duration": 0.022141,
     "end_time": "2021-06-10T12:19:50.779137",
     "exception": false,
     "start_time": "2021-06-10T12:19:50.756996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## 2. ü§ñ **Training the model**\n",
    "\n",
    "MLP Mixer is designed originally for image classification problems, as observed in their [research paper](https://arxiv.org/abs/2105.01601). We modify the architecture and produce patches from embeddings of shape `( num_patches , embedding_dims )`. In context of textual data, `embedding_dims` could be thought as `num_channels` in an image. \n",
    "\n",
    "The rest of the components including MLPs and Mixer layers remain as they are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "informed-packing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T12:19:50.829048Z",
     "iopub.status.busy": "2021-06-10T12:19:50.828221Z",
     "iopub.status.idle": "2021-06-10T12:19:50.838865Z",
     "shell.execute_reply": "2021-06-10T12:19:50.839446Z",
     "shell.execute_reply.started": "2021-06-10T11:48:17.49612Z"
    },
    "papermill": {
     "duration": 0.036862,
     "end_time": "2021-06-10T12:19:50.839679",
     "exception": false,
     "start_time": "2021-06-10T12:19:50.802817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Multilayer Perceptron with GeLU ( Gaussian Linear Units ) activation\n",
    "def mlp( x , hidden_dims ):\n",
    "    y = tf.keras.layers.Dense( hidden_dims )( x )\n",
    "    y = tf.nn.gelu( y )\n",
    "    y = tf.keras.layers.Dense( x.shape[ -1 ] )( y )\n",
    "    y = tf.keras.layers.Dropout( 0.4 )( y )\n",
    "    return y\n",
    "\n",
    "# Token Mixing MLPs : Allow communication within tokens ( patches ) or, intuitively, between different parts\n",
    "# of the same sequence.\n",
    "def token_mixing( x , token_mixing_mlp_dims ):\n",
    "    # x is a tensor of shape ( batch_size , num_patches , channels )\n",
    "    x = tf.keras.layers.LayerNormalization( epsilon=1e-6 )( x )\n",
    "    x = tf.keras.layers.Permute( dims=[ 2 , 1 ] )( x ) \n",
    "    # After transposition, shape of x -> ( batch_size , channels , num_patches )\n",
    "    x = mlp( x , token_mixing_mlp_dims )\n",
    "    return x\n",
    "\n",
    "# Channel Mixing MLPs : Allow communication within channels ( features of embeddings )\n",
    "def channel_mixing( x , channel_mixing_mlp_dims ):\n",
    "    # x is a tensor of shape ( batch_size , num_patches , channels )\n",
    "    x = tf.keras.layers.LayerNormalization( epsilon=1e-6 )( x )\n",
    "    x = mlp( x , channel_mixing_mlp_dims )\n",
    "    return x\n",
    "\n",
    "# Mixer layer consisting of token mixing MLPs and channel mixing MLPs\n",
    "# input shape -> ( batch_size , channels , num_patches )\n",
    "# output shape -> ( batch_size , channels , num_patches )\n",
    "def mixer( x , token_mixing_mlp_dims , channel_mixing_mlp_dims ):\n",
    "    # inputs x of are of shape ( batch_size , num_patches , channels )\n",
    "    # Note: \"channels\" is used instead of \"embedding_dims\"\n",
    "    \n",
    "    # Add token mixing MLPs\n",
    "    token_mixing_out = token_mixing( x , token_mixing_mlp_dims )\n",
    "    # Shape of token_mixing_out -> ( batch_size , channels , num_patches )\n",
    "\n",
    "    token_mixing_out = tf.keras.layers.Permute( dims=[ 2 , 1 ] )( token_mixing_out )\n",
    "    # Shape of transposition -> ( batch_size , num_patches , channels )\n",
    "    \n",
    "    #  Add skip connection\n",
    "    token_mixing_out = tf.keras.layers.Add()( [ x , token_mixing_out ] )\n",
    "\n",
    "    # Add channel mixing MLPs\n",
    "    channel_mixing_out = channel_mixing( token_mixing_out , channel_mixing_mlp_dims )\n",
    "    # Shape of channel_mixing_out -> ( batch_size , num_patches , channels )\n",
    "    \n",
    "    # Add skip connection\n",
    "    channel_mixing_out = tf.keras.layers.Add()( [ channel_mixing_out , token_mixing_out ] )\n",
    "    # Shape of channel_mixing_out -> ( batch_size , num_patches , channels )\n",
    "\n",
    "    return channel_mixing_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-oliver",
   "metadata": {
    "papermill": {
     "duration": 0.02239,
     "end_time": "2021-06-10T12:19:50.884479",
     "exception": false,
     "start_time": "2021-06-10T12:19:50.862089",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Compile the model with `SparseCategoricalFocalLoss` and `Adam` optimizer. We'll use the class weights, which we computed earlier, in the `model.fit()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "centered-horror",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T12:19:50.932987Z",
     "iopub.status.busy": "2021-06-10T12:19:50.932168Z",
     "iopub.status.idle": "2021-06-10T12:19:52.082520Z",
     "shell.execute_reply": "2021-06-10T12:19:52.083118Z",
     "shell.execute_reply.started": "2021-06-10T12:06:45.352863Z"
    },
    "papermill": {
     "duration": 1.176214,
     "end_time": "2021-06-10T12:19:52.083383",
     "exception": false,
     "start_time": "2021-06-10T12:19:50.907169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 23)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 23, 64)       1086208     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 4, 64)        20480       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 4, 64)        128         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, 64, 4)        0           layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64, 32)       160         permute[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu (TFOpLambda)         (None, 64, 32)       0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64, 4)        132         tf.nn.gelu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64, 4)        0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 4, 64)        0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 4, 64)        0           conv1d[0][0]                     \n",
      "                                                                 permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 4, 64)        128         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4, 64)        4160        layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu_1 (TFOpLambda)       (None, 4, 64)        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4, 64)        4160        tf.nn.gelu_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4, 64)        0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 4, 64)        0           dropout_1[0][0]                  \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 4, 64)        128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 64, 4)        0           layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64, 32)       160         permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu_2 (TFOpLambda)       (None, 64, 32)       0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64, 4)        132         tf.nn.gelu_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64, 4)        0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, 4, 64)        0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 4, 64)        0           add_1[0][0]                      \n",
      "                                                                 permute_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 4, 64)        128         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4, 64)        4160        layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu_3 (TFOpLambda)       (None, 4, 64)        0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 4, 64)        4160        tf.nn.gelu_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 4, 64)        0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 4, 64)        0           dropout_3[0][0]                  \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 4, 64)        128         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, 64, 4)        0           layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64, 32)       160         permute_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu_4 (TFOpLambda)       (None, 64, 32)       0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 64, 4)        132         tf.nn.gelu_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64, 4)        0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "permute_5 (Permute)             (None, 4, 64)        0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 4, 64)        0           add_3[0][0]                      \n",
      "                                                                 permute_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 4, 64)        128         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 4, 64)        4160        layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu_5 (TFOpLambda)       (None, 4, 64)        0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 4, 64)        4160        tf.nn.gelu_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 4, 64)        0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 4, 64)        0           dropout_5[0][0]                  \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 4, 64)        128         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "permute_6 (Permute)             (None, 64, 4)        0           layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 64, 32)       160         permute_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu_6 (TFOpLambda)       (None, 64, 32)       0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 64, 4)        132         tf.nn.gelu_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64, 4)        0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "permute_7 (Permute)             (None, 4, 64)        0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 4, 64)        0           add_5[0][0]                      \n",
      "                                                                 permute_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 4, 64)        128         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 4, 64)        4160        layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu_7 (TFOpLambda)       (None, 4, 64)        0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 4, 64)        4160        tf.nn.gelu_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 4, 64)        0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 4, 64)        0           dropout_7[0][0]                  \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, 4, 64)        128         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "permute_8 (Permute)             (None, 64, 4)        0           layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 64, 32)       160         permute_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu_8 (TFOpLambda)       (None, 64, 32)       0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 64, 4)        132         tf.nn.gelu_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64, 4)        0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "permute_9 (Permute)             (None, 4, 64)        0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 4, 64)        0           add_7[0][0]                      \n",
      "                                                                 permute_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, 4, 64)        128         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 4, 64)        4160        layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu_9 (TFOpLambda)       (None, 4, 64)        0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 4, 64)        4160        tf.nn.gelu_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 4, 64)        0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 4, 64)        0           dropout_9[0][0]                  \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 4, 64)        128         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "permute_10 (Permute)            (None, 64, 4)        0           layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 64, 32)       160         permute_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu_10 (TFOpLambda)      (None, 64, 32)       0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 64, 4)        132         tf.nn.gelu_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 64, 4)        0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "permute_11 (Permute)            (None, 4, 64)        0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 4, 64)        0           add_9[0][0]                      \n",
      "                                                                 permute_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 4, 64)        128         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 4, 64)        4160        layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu_11 (TFOpLambda)      (None, 4, 64)        0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 4, 64)        4160        tf.nn.gelu_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 4, 64)        0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 4, 64)        0           dropout_11[0][0]                 \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 4, 64)        128         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "permute_12 (Permute)            (None, 64, 4)        0           layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 64, 32)       160         permute_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu_12 (TFOpLambda)      (None, 64, 32)       0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 64, 4)        132         tf.nn.gelu_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 64, 4)        0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "permute_13 (Permute)            (None, 4, 64)        0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 4, 64)        0           add_11[0][0]                     \n",
      "                                                                 permute_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 4, 64)        128         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 4, 64)        4160        layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu_13 (TFOpLambda)      (None, 4, 64)        0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 4, 64)        4160        tf.nn.gelu_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 4, 64)        0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 4, 64)        0           dropout_13[0][0]                 \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, 4, 64)        128         add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "permute_14 (Permute)            (None, 64, 4)        0           layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 64, 32)       160         permute_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu_14 (TFOpLambda)      (None, 64, 32)       0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 64, 4)        132         tf.nn.gelu_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 64, 4)        0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "permute_15 (Permute)            (None, 4, 64)        0           dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 4, 64)        0           add_13[0][0]                     \n",
      "                                                                 permute_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, 4, 64)        128         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 4, 64)        4160        layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.gelu_15 (TFOpLambda)      (None, 4, 64)        0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 4, 64)        4160        tf.nn.gelu_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 4, 64)        0           dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 4, 64)        0           dropout_15[0][0]                 \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, 4, 64)        128         add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 64)           0           layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 2)            130         global_average_pooling1d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 1,177,890\n",
      "Trainable params: 1,157,410\n",
      "Non-trainable params: 20,480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# These hyperparameters were searched with KerasTuner\n",
    "embedding_dims = 64\n",
    "token_mixing_mlp_dims = 32\n",
    "channel_mixing_mlp_dims = 64\n",
    "patch_size = 5\n",
    "num_mixer_layers = 8\n",
    "learning_rate = 5e-3\n",
    "\n",
    "num_classes = 2\n",
    "seq_input_shape = ( maxlen , )\n",
    "    \n",
    "# Model input layer\n",
    "inputs = tf.keras.layers.Input( shape=seq_input_shape )\n",
    "\n",
    "# Embedding layer which converts int sequences into dense vectors\n",
    "embedding = tf.keras.layers.Embedding( input_dim=vocab_size + 1 , output_dim=embedding_dims , input_length=maxlen )( inputs )\n",
    "    \n",
    "# Conv1D layer to produce patches from given sequences. \n",
    "patches = tf.keras.layers.Conv1D( embedding_dims ,\n",
    "                                 kernel_size=patch_size ,\n",
    "                                 strides=patch_size ,\n",
    "                                 use_bias=False , \n",
    "                                 trainable=False )( embedding )\n",
    "    \n",
    "x = patches\n",
    "for _ in range( num_mixer_layers ):\n",
    "    x = mixer( x , token_mixing_mlp_dims , channel_mixing_mlp_dims )\n",
    "        \n",
    "x = tf.keras.layers.LayerNormalization( epsilon=1e-6 )( x )\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()( x )\n",
    "outputs = tf.keras.layers.Dense( num_classes , activation='softmax' )( x )\n",
    "\n",
    "model = tf.keras.models.Model( inputs , outputs )\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "waiting-manhattan",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T12:19:52.148934Z",
     "iopub.status.busy": "2021-06-10T12:19:52.148181Z",
     "iopub.status.idle": "2021-06-10T12:20:54.321989Z",
     "shell.execute_reply": "2021-06-10T12:20:54.322655Z",
     "shell.execute_reply.started": "2021-06-10T02:40:06.694587Z"
    },
    "papermill": {
     "duration": 62.214593,
     "end_time": "2021-06-10T12:20:54.322887",
     "exception": false,
     "start_time": "2021-06-10T12:19:52.108294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "191/191 [==============================] - 22s 64ms/step - loss: 0.7295 - accuracy: 0.5341 - val_loss: 0.5815 - val_accuracy: 0.7144\n",
      "Epoch 2/5\n",
      "191/191 [==============================] - 10s 53ms/step - loss: 0.3319 - accuracy: 0.8671 - val_loss: 0.6996 - val_accuracy: 0.7288\n",
      "Epoch 3/5\n",
      "191/191 [==============================] - 10s 53ms/step - loss: 0.1030 - accuracy: 0.9672 - val_loss: 0.9517 - val_accuracy: 0.7236\n",
      "Epoch 4/5\n",
      "191/191 [==============================] - 10s 53ms/step - loss: 0.0562 - accuracy: 0.9798 - val_loss: 1.0100 - val_accuracy: 0.7360\n",
      "Epoch 5/5\n",
      "191/191 [==============================] - 10s 53ms/step - loss: 0.0456 - accuracy: 0.9797 - val_loss: 1.3630 - val_accuracy: 0.7216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff6e4539f90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Batch size and epochs\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "\n",
    "# Compile the model and start the training\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy' , \n",
    "    optimizer=tf.keras.optimizers.Adam( learning_rate ) ,\n",
    "    metrics=[ 'accuracy' ]\n",
    ")\n",
    "\n",
    "model.fit(train_x ,\n",
    "          train_y ,\n",
    "          batch_size=batch_size ,\n",
    "          validation_data=( test_x , test_y ) ,\n",
    "          epochs=num_epochs , \n",
    "          class_weight=class_weights ,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-member",
   "metadata": {
    "papermill": {
     "duration": 0.217349,
     "end_time": "2021-06-10T12:20:54.756612",
     "exception": false,
     "start_time": "2021-06-10T12:20:54.539263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## 3. ü¶æ **Evaluating the model**\n",
    "\n",
    "![Evaluation Metrics](https://static.packt-cdn.com/products/9781785282287/graphics/B04223_10_02.jpg)\n",
    "\n",
    "> Image Source: [Computing precision, recall, and F1-score - Packt](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785282287/10/ch10lvl1sec133/computing-precision-recall-and-f1-score)\n",
    "\n",
    "After training our model, we'll like to evaluate it using our test data. Using `sklearn.metrics.classification_report` we examine the precision, recall and f1 scores for the two classes, `disaster` and `not disaster`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "modified-scientist",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T12:20:55.193680Z",
     "iopub.status.busy": "2021-06-10T12:20:55.193018Z",
     "iopub.status.idle": "2021-06-10T12:20:57.207666Z",
     "shell.execute_reply": "2021-06-10T12:20:57.207021Z",
     "shell.execute_reply.started": "2021-06-10T02:46:40.782172Z"
    },
    "papermill": {
     "duration": 2.234377,
     "end_time": "2021-06-10T12:20:57.207840",
     "exception": false,
     "start_time": "2021-06-10T12:20:54.973463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "not disaster       0.79      0.72      0.75       898\n",
      "    disaster       0.64      0.72      0.68       625\n",
      "\n",
      "    accuracy                           0.72      1523\n",
      "   macro avg       0.72      0.72      0.72      1523\n",
      "weighted avg       0.73      0.72      0.72      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fetch model predictions for test_x\n",
    "pred_y = model.predict( test_x )\n",
    "\n",
    "# Print the classification report\n",
    "report = sklearn.metrics.classification_report( np.argmax( test_y , axis=1 ) , np.argmax( pred_y , axis=1 ) , target_names=[ 'not disaster' , 'disaster' ] )\n",
    "print( report )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-asian",
   "metadata": {
    "papermill": {
     "duration": 0.220305,
     "end_time": "2021-06-10T12:20:57.651163",
     "exception": false,
     "start_time": "2021-06-10T12:20:57.430858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Alongside, we plot the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "nominated-serbia",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T12:20:58.096440Z",
     "iopub.status.busy": "2021-06-10T12:20:58.095820Z",
     "iopub.status.idle": "2021-06-10T12:20:58.309746Z",
     "shell.execute_reply": "2021-06-10T12:20:58.309218Z",
     "shell.execute_reply.started": "2021-06-10T02:46:22.634565Z"
    },
    "papermill": {
     "duration": 0.439115,
     "end_time": "2021-06-10T12:20:58.309896",
     "exception": false,
     "start_time": "2021-06-10T12:20:57.870781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ff6c7611790>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot the confusion matrix\n",
    "conf_matrix = sklearn.metrics.confusion_matrix( np.argmax( test_y , axis=1 ) ,np.argmax( pred_y , axis=1 ) )\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay( conf_matrix , display_labels=[ 'not disaster' , 'disaster' ] )\n",
    "disp.plot() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-numbers",
   "metadata": {
    "papermill": {
     "duration": 0.216608,
     "end_time": "2021-06-10T12:20:58.744660",
     "exception": false,
     "start_time": "2021-06-10T12:20:58.528052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## 4. ‚úçÔ∏è **Submitting the results**\n",
    "\n",
    "For submitting our predictions to the competition, we need to write them in a CSV file called `submission.csv`. We parse the tweets from `test.csv`, clean them and generate perdictions for them. These predictions are stored with their corresponding ids in `submission.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "economic-philippines",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T12:20:59.193344Z",
     "iopub.status.busy": "2021-06-10T12:20:59.192707Z",
     "iopub.status.idle": "2021-06-10T12:21:13.221125Z",
     "shell.execute_reply": "2021-06-10T12:21:13.221703Z",
     "shell.execute_reply.started": "2021-06-10T02:46:51.015005Z"
    },
    "papermill": {
     "duration": 14.258733,
     "end_time": "2021-06-10T12:21:13.221894",
     "exception": false,
     "start_time": "2021-06-10T12:20:58.963161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the test.csv file\n",
    "test_df = pd.read_csv( '../input/nlp-getting-started/test.csv' , usecols=[ 'text' , 'id' ] )\n",
    "\n",
    "# Clean, tokenize, pad the sentences from test_df\n",
    "test_inputs = []\n",
    "for text in test_df[ 'text' ].values:\n",
    "    tokens = process_sent( text )\n",
    "    out = sent_to_int_seq( tokens )\n",
    "    out = pad_sequence( out , maxlen )\n",
    "    test_inputs.append( out )\n",
    "    \n",
    "# Fetch predictions for test_inputs\n",
    "test_inputs = np.array( test_inputs )\n",
    "predicted_labels = np.argmax( model.predict( test_inputs ) , axis=1 )\n",
    "    \n",
    "ids = test_df[ 'id' ].values\n",
    "\n",
    "# Create the submission.csv file from ids and predicted_labels\n",
    "submission_csv = { 'id' : ids , 'target' : predicted_labels }\n",
    "submission_csv = pd.DataFrame.from_dict( submission_csv )\n",
    "submission_csv.to_csv( 'submission.csv' , index=False )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 129.92491,
   "end_time": "2021-06-10T12:21:16.265037",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-10T12:19:06.340127",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
